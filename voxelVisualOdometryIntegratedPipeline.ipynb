{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd01b74fdc6a0069e63d7c35520416cfe241b2e1d296eedc56ee9c4fe2929446925",
   "display_name": "Python 3.6.8  ('bot': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "1b74fdc6a0069e63d7c35520416cfe241b2e1d296eedc56ee9c4fe2929446925"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Voxel Visual Odometry Integrated Pipeline\n",
    "## Importing requirements and scripts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ImageProcessor import ImageProcessor\n",
    "from StereoMatcher import StereoMatcher\n",
    "from helperScripts.TimeKeeper import TimeKeeper\n",
    "from VoxelGrid import VoxelGrid\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "source": [
    "## Load calibrations and other data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading from data/monoCalibration.json\nLoaded mono calibration\nReading from data/stereoCalibration.json\nLoaded stereo calibration\nReading from data/cameraProperties.json\nLoaded camera properties\nReading from data/stereoRectify.json\nLoaded stereo rectification data\n"
     ]
    }
   ],
   "source": [
    "imageProcessor = ImageProcessor()\n",
    "imageProcessor.verbose = True\n",
    "imageProcessor.loadMonoCalibration()\n",
    "imageProcessor.loadStereoCalibration()\n",
    "imageProcessor.loadCameraProperties()\n",
    "imageProcessor.loadStereoRectify()\n",
    "imageProcessor.initUndistortRectifyMap()"
   ]
  },
  {
   "source": [
    "## Create stereo matcher"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading from data/parametersSGBM.json\n"
     ]
    }
   ],
   "source": [
    "stereoMatcher = StereoMatcher(imageProcessor=imageProcessor, \\\n",
    "                matcher=\"SGBM\", vertical=True, createRightMatcher=False)"
   ]
  },
  {
   "source": [
    "## Create voxel grid"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxelGrid = VoxelGrid(stereoMatcher=stereoMatcher, imageProcessor=imageProcessor)\n",
    "voxelGrid.verbose = True"
   ]
  },
  {
   "source": [
    "## TimeKeeper for performance metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeKeeper = TimeKeeper()"
   ]
  },
  {
   "source": [
    "## Loading images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selections: 0-13\n"
     ]
    }
   ],
   "source": [
    "path = \"testImages/visualOdometryTestImages\"\n",
    "folderChoice = 2\n",
    "\n",
    "path = \"\".join([path, \"/\", str(folderChoice)])\n",
    "\n",
    "imageGlobL = sorted(glob.glob(\"\".join([path, \"/top_*\", \".png\"])))\n",
    "imageGlobR = sorted(glob.glob(\"\".join([path, \"/bottom_*\", \".png\"])))\n",
    "\n",
    "if not len(imageGlobL)==len(imageGlobR):\n",
    "    print(\"Images could not be matched\")\n",
    "\n",
    "print (\"Selections: 0-{}\".format(len(imageGlobL)-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15 15\n"
     ]
    }
   ],
   "source": [
    "# Top images\n",
    "imagesL = []\n",
    "for imageFile in imageGlobL:\n",
    "    imagesL.append(cv2.imread(imageFile))\n",
    "\n",
    "# Bottom images\n",
    "imagesR = []\n",
    "for imageFile in imageGlobR:\n",
    "    imagesR.append(cv2.imread(imageFile))\n",
    "\n",
    "print(len(imagesL), len(imagesR))"
   ]
  },
  {
   "source": [
    "### View image pair"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "150d57aa674743768f398c808eb1fa9c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c609c0afd0>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "selection = 0\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Image pair\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "                    np.hstack([imagesL[selection], imagesR[selection]]), \\\n",
    "                cv2.ROTATE_90_CLOCKWISE), \\\n",
    "            cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "caa50346980a416f801f022dfb54373d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c609b35eb8>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Sequential image pair\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "                    np.hstack([imagesL[selection], imagesL[selection+1]]), \\\n",
    "                cv2.ROTATE_90_CLOCKWISE), \\\n",
    "            cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "# Visual Odometry\n",
    "## Feature extraction\n",
    "### Initializing feature extractor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features detected in frame 0: 500\nCoordinates of first keypoint in frame 0: (288.0, 130.0)\n"
     ]
    }
   ],
   "source": [
    "# Extraction function\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "def extractFeatures(image, orb=orb):\n",
    "    \"\"\"Find keypoints and descriptors for the image\"\"\"\n",
    "    keypoints = orb.detect(image, None)\n",
    "    keypoints, descriptors = orb.compute(image, keypoints)\n",
    "    \n",
    "    return keypoints, descriptors\n",
    "\n",
    "keypoints0, descriptors0 = extractFeatures(imagesL[selection])\n",
    "keypoints1, descriptors1 = extractFeatures(imagesL[selection+1])\n",
    "\n",
    "print(\"Number of features detected in frame {}: {}\"\\\n",
    "                                .format(selection, len(keypoints0)))\n",
    "print(\"Coordinates of first keypoint in frame {}: {}\"\\\n",
    "                                .format(selection, str(keypoints0[0].pt)))"
   ]
  },
  {
   "source": [
    "### Visualize extracted features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "161102d5d20b462f9335cd4de3f26bb9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4655f0ddf924bfdb18e924650bc5d24"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def visualizeFeatures(image, keypoints, flag):\n",
    "    \"\"\"Visualize extracted features in image\"\"\"\n",
    "    display = cv2.drawKeypoints(image, keypoints, None, flags=flag)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.imshow(cv2.cvtColor\\\n",
    "                (cv2.rotate(\\\n",
    "                    display, cv2.ROTATE_90_CLOCKWISE), \\\n",
    "                cv2.COLOR_BGR2RGB))\n",
    "\n",
    "visualizeFeatures(imagesL[selection], keypoints0, 4)\n",
    "visualizeFeatures(imagesL[selection], keypoints0, 2)"
   ]
  },
  {
   "source": [
    "## Initialize feature matcher"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features matched in frames 0 and 1: 100\n"
     ]
    }
   ],
   "source": [
    "bfMatcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "def matchFeatures(descriptors0, descriptors1, \\\n",
    "                                bfMatcher=bfMatcher, bestNMatches=100):\n",
    "    \"\"\"Match features from two images\"\"\"\n",
    "    match = bfMatcher.match(descriptors0, descriptors1)\n",
    "    match = sorted(match, key = lambda x:x.distance)\n",
    "\n",
    "    return match[:bestNMatches]\n",
    "\n",
    "match01 = matchFeatures(descriptors0, descriptors1, bfMatcher)\n",
    "\n",
    "print(\"Number of features matched in frames {} and {}: {}\"\\\n",
    "                        .format(selection, selection+1, len(match01)))"
   ]
  },
  {
   "source": [
    "### Visualize feature match"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d7009f93f31d41d5b4e82e85bd076968"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def visualizeMatches(image0, keypoints0, image1, keypoints1, match):\n",
    "    imageMatches = cv2.drawMatches(image0, keypoints0, \\\n",
    "                            image1, keypoints1, match, None, flags=2)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "                    imageMatches, cv2.ROTATE_90_CLOCKWISE), \\\n",
    "                cv2.COLOR_BGR2RGB))\n",
    "\n",
    "visualizeMatches(imagesL[selection], keypoints0, \\\n",
    "                imagesL[selection+1], keypoints1, match01)"
   ]
  },
  {
   "source": [
    "## Trajectory Estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimated rotation:\n [[ 0.99924559  0.03707732 -0.01155537]\n [-0.03609422  0.99645086  0.07604537]\n [ 0.01433391 -0.07557091  0.9970374 ]]\nEstimated translation:\n [[ 0.04634577]\n [ 0.08936792]\n [-0.99491982]]\n"
     ]
    }
   ],
   "source": [
    "def estimateMotion(match, keypoints0, keypoints1, k=imageProcessor.cameraMatrixL):\n",
    "    \"\"\"Estimate camera motion from a pair of subsequent image frames\"\"\"\n",
    "    imagePoints0 = []\n",
    "    imagePoints1 = []\n",
    "\n",
    "    for m in match:\n",
    "        train_idx = m.trainIdx\n",
    "        query_idx = m.queryIdx\n",
    "\n",
    "        p1x, p1y = keypoints0[query_idx].pt \n",
    "        imagePoints0.append([p1x,p1y])\n",
    "\n",
    "        p2x,p2y = keypoints1[train_idx].pt \n",
    "        imagePoints1.append([p2x,p2y])\n",
    "    \n",
    "    E, mask = cv2.findEssentialMat(\\\n",
    "                    np.array(imagePoints0), np.array(imagePoints1), k, \\\n",
    "                    cv2.RANSAC, 0.999, 1.0) \n",
    "\n",
    "    retval, rmat, tvec, mask = cv2.recoverPose(E, np.array(imagePoints0), \\\n",
    "                    np.array(imagePoints1), k)\n",
    "\n",
    "    return rmat, tvec, imagePoints0, imagePoints1\n",
    "\n",
    "rmat, tvec, imagePoints0, imagePoints1 = estimateMotion(match01, \\\n",
    "                        keypoints0, keypoints1)\n",
    "\n",
    "print(\"Estimated rotation:\\n {0}\".format(rmat))\n",
    "print(\"Estimated translation:\\n {0}\".format(tvec))"
   ]
  },
  {
   "source": [
    "## Movement visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1d98668185548119fe529744465a94f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c60a910940>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "def visualizeCameraMovement(image0, imagePoints0, \\\n",
    "                image1, imagePoints1, showImageAfterMove=False):\n",
    "    \"\"\"Visualize camera movement across frames\"\"\"\n",
    "    image0 = image0.copy()\n",
    "    image1 = image1.copy()\n",
    "\n",
    "    for i in range(0, len(imagePoints0)):\n",
    "        # Coordinates of a point on t frame\n",
    "        p1 = (int(imagePoints0[i][0]), int(imagePoints0[i][1]))\n",
    "        # Coordinates of the same point on t+1 frame\n",
    "        p2 = (int(imagePoints1[i][0]), int(imagePoints1[i][1]))\n",
    "\n",
    "        cv2.circle(image0, p1, 5, (0, 255, 0), 1)\n",
    "        cv2.arrowedLine(image0, p1, p2, (0, 255, 0), 1)\n",
    "        cv2.circle(image0, p2, 5, (255, 0, 0), 1)\n",
    "\n",
    "        if showImageAfterMove:\n",
    "            cv2.circle(image1, p2, 5, (255, 0, 0), 1)\n",
    "    \n",
    "    if showImageAfterMove: \n",
    "        return image1\n",
    "    else:\n",
    "        return image0\n",
    "\n",
    "imageMovementBefore = visualizeCameraMovement(imagesL[selection], \\\n",
    "                    imagePoints0, imagesL[selection+1], imagePoints1)\n",
    "\n",
    "imageMovementAfter = visualizeCameraMovement(imagesL[selection], \\\n",
    "                    imagePoints0, imagesL[selection+1], imagePoints1, \\\n",
    "                    showImageAfterMove=True)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "                np.hstack([imageMovementBefore, imageMovementAfter]), \\\n",
    "                    cv2.ROTATE_90_CLOCKWISE), \\\n",
    "                cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "# Voxel grid\n",
    "## Creating disparity map\n",
    "### Convert to grayscale and undistort"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "845135c887cb490ea511bd2e15703dac"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a6229ac1c304538983a29300bf63b32"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cbef8dd35a04ed7a0005d8e3c4ca7ca"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c60ab926a0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "imageProcessor.convertToGrayscale(imagesL[selection], imagesR[selection])\n",
    "imageProcessor.undistortRectifyRemap(imageProcessor.grayImageL, \\\n",
    "                                        imageProcessor.grayImageR)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "fig.suptitle(\"left/right undistorted\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "            np.hstack([imageProcessor.undistortImageL, \\\n",
    "            imageProcessor.undistortImageR]), \\\n",
    "            cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "fig.suptitle(\"horizontal epipolar\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "        imageProcessor.drawHorEpipolarLines(\\\n",
    "        imageProcessor.undistortImageL, imageProcessor.undistortImageR), \\\n",
    "        cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "fig = plt.figure(figsize=(7,12))\n",
    "fig.suptitle(\"left/right undistorted\")\n",
    "plt.imshow(cv2.cvtColor(imageProcessor.drawVertEpipolarLines(\\\n",
    "        imageProcessor.undistortImageL, imageProcessor.undistortImageR), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### Compute disparity map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minDisparity: 9.0\nmaxDisparity: 41.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0596e0eb7ea94b41920baa5ebaa4bd21"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c60a2d36d8>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "stereoMatcher.computeDisparity(\\\n",
    "                grayImageL=imageProcessor.undistortImageL, \\\n",
    "                grayImageR=imageProcessor.undistortImageR)\n",
    "\n",
    "stereoMatcher.clampDisparity()\n",
    "stereoMatcher.applyClosingFilter()\n",
    "\n",
    "print (\"minDisparity:\", stereoMatcher.disparityMapL.min())\n",
    "print (\"maxDisparity:\", stereoMatcher.disparityMapL.max())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cv2.rotate(stereoMatcher.disparityMapL, \\\n",
    "    cv2.ROTATE_90_CLOCKWISE))"
   ]
  },
  {
   "source": [
    "### Compute depth map (not necessary)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9.0\n41.0\n(640, 360)\n481.33795\n2192.7617\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e10d60b52e8847fa94131aa46643d4a7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c61d6580b8>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "focalLength = imageProcessor.projectionMatrixL[0][0] # changes with rectify?\n",
    "baseline = 32 # mm, measured irl\n",
    "\n",
    "stereoMatcher.disparityMapL[stereoMatcher.disparityMapL==0] = 0.9\n",
    "stereoMatcher.disparityMapL[stereoMatcher.disparityMapL==-1] = 0.9\n",
    "\n",
    "depthMap = np.empty_like(stereoMatcher.disparityMapL)\n",
    "depthMap = (focalLength*baseline)/stereoMatcher.disparityMapL[:]\n",
    "\n",
    "print (stereoMatcher.disparityMapL.min())\n",
    "print (stereoMatcher.disparityMapL.max())\n",
    "print (depthMap.shape)\n",
    "print (depthMap.min())\n",
    "print (depthMap.max())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cv2.rotate(depthMap, cv2.ROTATE_90_CLOCKWISE))"
   ]
  },
  {
   "source": [
    "## Compute voxel grid\n",
    "### Compute, filter, and rotate point cloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Points in unfiltered pointcloud: 9600\nPoints in filtered pointcloud: 6214\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "551d6071ca9f4b8f99684cd92ab743d5"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "pointSubsample = 24\n",
    "voxelSize = 100\n",
    "voxelStopFraction = 10\n",
    "occupancyThreshold = 10\n",
    "\n",
    "def generatePointCloud(disparityMapL, dispToDepthMatrix):\n",
    "    points = cv2.reprojectImageTo3D(\\\n",
    "            disparityMapL, \\\n",
    "            dispToDepthMatrix)\n",
    "\n",
    "    # Reshaping to a list of 3D coordinates\n",
    "    pointCloud = points.reshape(\\\n",
    "                (points.shape[0]*points.shape[1],3))[0::pointSubsample]\\\n",
    "                                                .astype(np.int16)\n",
    "\n",
    "    return pointCloud\n",
    "\n",
    "pointCloud = generatePointCloud(stereoMatcher.disparityMapL,\\\n",
    "                                imageProcessor.dispToDepthMatrix)\n",
    "\n",
    "print(\"Points in unfiltered pointcloud: {}\".format(pointCloud.shape[0]))\n",
    "\n",
    "def filterPointCloud(pointCloud):\n",
    "    # Filtering x values\n",
    "    pointCloud = pointCloud[np.logical_and(\\\n",
    "            pointCloud[:, 0]>pointCloud[:, 0].min(), \\\n",
    "            pointCloud[:, 0]<pointCloud[:, 0].max())]\n",
    "    # Filtering y values\n",
    "    pointCloud = pointCloud[np.logical_and(\\\n",
    "            pointCloud[:, 1]>pointCloud[:, 1].min(), \\\n",
    "            pointCloud[:, 1]<pointCloud[:, 1].max())]\n",
    "    # Filtering z values\n",
    "    pointCloud = pointCloud[np.logical_and(\\\n",
    "            pointCloud[:, 2]>pointCloud[:, 2].min(), \\\n",
    "            pointCloud[:, 2]<pointCloud[:, 2].max())]\n",
    "\n",
    "    return pointCloud\n",
    "\n",
    "pointCloud = filterPointCloud(pointCloud)\n",
    "\n",
    "print(\"Points in filtered pointcloud: {}\".format(pointCloud.shape[0]))\n",
    "\n",
    "redefineRotationMatrix = np.array([ [ 0,  0, -1],\n",
    "                                    [ 0,  1,  0],\n",
    "                                    [ 1,  0,  0] ])\n",
    "\n",
    "def rotatePointCloud(pointCloud, rotationMatrix):\n",
    "    pointCloud = np.dot(pointCloud[:], rotationMatrix)\n",
    "\n",
    "    return pointCloud\n",
    "\n",
    "pointCloud = rotatePointCloud(pointCloud, redefineRotationMatrix)\n",
    "\n",
    "def plotGrid(grid, s):\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(111, projection = \"3d\")\n",
    "\n",
    "    ax.scatter(grid[:,0], grid[:,1], grid[:,2], s=s)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_zlabel(\"$z$\")\n",
    "\n",
    "    # Camera axis begins at x=0 and looks to positive x\n",
    "    ax.set_xlim(0,2000)\n",
    "    ax.set_ylim(-1000,1000)\n",
    "    ax.set_zlim(-1000,1000)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plotGrid(pointCloud, 1)"
   ]
  },
  {
   "source": [
    "### Voxelize point cloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Voxels in grid: 54; 64 iterations\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf05691e496346afa98d328335590577"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "newVoxelGrid = None\n",
    "\n",
    "def voxelizePointCloud(pointCloud, voxelSize, \\\n",
    "                        occupancyThreshold, voxelStopFraction):\n",
    "    iterations = 0\n",
    "    newVoxelGrid = []\n",
    "    initialSize = pointCloud.shape[0]\n",
    "    remainingPoints = initialSize\n",
    "    samplingLimit = np.zeros_like(pointCloud[0])\n",
    "        \n",
    "    while remainingPoints>(initialSize/voxelStopFraction):\n",
    "\n",
    "        sampledPoint = pointCloud[np.random.randint(0,remainingPoints)]\n",
    "\n",
    "        for n in range(3):\n",
    "            samplingLimit[n]=\\\n",
    "                (sampledPoint[n]//voxelSize)*voxelSize\n",
    "\n",
    "        mask = np.ones(remainingPoints, dtype=bool)\n",
    "\n",
    "        for n in range(len(sampledPoint)):\n",
    "            mask = np.logical_and(mask, np.logical_and(\\\n",
    "                pointCloud[:,n]>=samplingLimit[n], \\\n",
    "                pointCloud[:,n]<samplingLimit[n]+voxelSize))\n",
    "\n",
    "        pointsInVoxel = pointCloud[mask]\n",
    "\n",
    "        if len(pointsInVoxel)>occupancyThreshold:\n",
    "            voxelMidpoint = samplingLimit+voxelSize/2\n",
    "            newVoxelGrid.append(voxelMidpoint)\n",
    "\n",
    "        pointCloud = pointCloud[np.invert(mask)]\n",
    "\n",
    "        iterations+=1\n",
    "\n",
    "        remainingPoints = pointCloud.shape[0]\n",
    "\n",
    "    newVoxelGrid = np.array(newVoxelGrid, dtype=np.int16)\n",
    "\n",
    "    return newVoxelGrid, iterations\n",
    "\n",
    "newVoxelGrid, iterations = voxelizePointCloud(pointCloud, voxelSize, \\\n",
    "                        occupancyThreshold, voxelStopFraction)\n",
    "\n",
    "print(\"\".join([\"Voxels in grid: {}; {} iterations\"]).format(\\\n",
    "                newVoxelGrid.shape[0], iterations))\n",
    "\n",
    "plotGrid(newVoxelGrid, voxelSize)"
   ]
  },
  {
   "source": [
    "## Refine voxel grid\n",
    "### Fetching camera fov"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Horizontal: 24.915932084055157\nVertical: 14.867033874651561\n"
     ]
    }
   ],
   "source": [
    "# Horizontal field of view (degrees)\n",
    "fovH = ((imageProcessor.fovYL+imageProcessor.fovYR)/4)*np.pi/180\n",
    "fovH -= fovH/8\n",
    "print(\"Horizontal:\", fovH*180/np.pi)\n",
    "\n",
    "# Vertical field of view (degrees)\n",
    "fovV = ((imageProcessor.fovXL+imageProcessor.fovXR)/4)*np.pi/180\n",
    "fovV -= fovV/8\n",
    "print(\"Vertical:\", fovV*180/np.pi)"
   ]
  },
  {
   "source": [
    "### Get voxels near camera"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnVoxelsInRange(voxelGrid, cameraPosition, distance=1500):\n",
    "    \"\"\"Return voxels that are within given distance from cameraPosition \n",
    "    in voxelGrid, along with yaw and distance\"\"\"\n",
    "    translatedVoxels = voxelGrid-cameraPosition\n",
    "    distanceToVoxels = np.linalg.norm(translatedVoxels, axis=1)\n",
    "\n",
    "    voxelsWithinRange = voxelGrid[distanceToVoxels<=distance]\n",
    "\n",
    "    distanceToVoxelsInRange = distanceToVoxels[distanceToVoxels<=distance]\n",
    "    yawToVoxelsInRange = np.arctan2(translatedVoxels[:,1], \\\n",
    "                                        translatedVoxels[:,0])\n",
    "\n",
    "    return voxelsWithinRange, yawToVoxelsInRange, distanceToVoxelsInRange"
   ]
  },
  {
   "source": [
    "### Get yaw of camera"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCameraYawRange(rotationMatrix, currentRotationMatrix=None):\n",
    "    \"\"\"Return camera yaw due to rotationMatrix on currentRotationMatrix\"\"\"\n",
    "    cameraDirectionVector = np.array([0,0,100])\n",
    "    \n",
    "    if currentRotationMatrix is None:\n",
    "        currentRotationMatrix = np.array(  [[1,0,0],\n",
    "                                            [0,1,0],\n",
    "                                            [0,0,1]]  )\n",
    "    \n",
    "    rotation = np.matmul(currentRotationMatrix, rotationMatrix)\n",
    "\n",
    "    cameraDirectionVector = np.dot(cameraDirectionVector, rotation)\n",
    "\n",
    "    cameraYaw = np.arctan2(cameraDirectionVector[1], cameraDirectionVector[0])\n",
    "\n",
    "    cameraYawRange = np.array([cameraYaw+fovH, cameraYaw-fovH])\n",
    "    # Wrapping around values at -180, 180 degrees\n",
    "    for n in range(len(cameraYawRange)):\n",
    "        if cameraYawRange[n]>np.pi:\n",
    "            cameraYawRange[n] -= 2*np.pi\n",
    "        if cameraYawRange[n]<=-np.pi:\n",
    "            cameraYawRange[n] += 2*np.pi\n",
    "    cameraYawRange = np.sort(cameraYawRange)[::-1]\n",
    "\n",
    "    return cameraYawRange, cameraYaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}