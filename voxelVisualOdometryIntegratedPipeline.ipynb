{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd01b74fdc6a0069e63d7c35520416cfe241b2e1d296eedc56ee9c4fe2929446925",
   "display_name": "Python 3.6.8  ('bot': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "1b74fdc6a0069e63d7c35520416cfe241b2e1d296eedc56ee9c4fe2929446925"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Voxel Visual Odometry Integrated Pipeline\n",
    "## Importing requirements and scripts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from ImageProcessor import ImageProcessor\n",
    "from StereoMatcher import StereoMatcher\n",
    "from helperScripts.TimeKeeper import TimeKeeper\n",
    "from VoxelGrid import VoxelGrid\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "source": [
    "## Load calibrations and other data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading from data/monoCalibration.json\nLoaded mono calibration\nReading from data/stereoCalibration.json\nLoaded stereo calibration\nReading from data/cameraProperties.json\nLoaded camera properties\nReading from data/stereoRectify.json\nLoaded stereo rectification data\n"
     ]
    }
   ],
   "source": [
    "imageProcessor = ImageProcessor()\n",
    "imageProcessor.verbose = True\n",
    "imageProcessor.loadMonoCalibration()\n",
    "imageProcessor.loadStereoCalibration()\n",
    "imageProcessor.loadCameraProperties()\n",
    "imageProcessor.loadStereoRectify()\n",
    "imageProcessor.initUndistortRectifyMap()"
   ]
  },
  {
   "source": [
    "## Create stereo matcher"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading from data/parametersSGBM.json\n"
     ]
    }
   ],
   "source": [
    "stereoMatcher = StereoMatcher(imageProcessor=imageProcessor, \\\n",
    "                matcher=\"SGBM\", vertical=True, createRightMatcher=False)"
   ]
  },
  {
   "source": [
    "## Create voxel grid"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxelGrid = VoxelGrid(stereoMatcher=stereoMatcher, imageProcessor=imageProcessor)\n",
    "voxelGrid.verbose = True"
   ]
  },
  {
   "source": [
    "## TimeKeeper for performance metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeKeeper = TimeKeeper()"
   ]
  },
  {
   "source": [
    "## Loading images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selections: 0-13\n"
     ]
    }
   ],
   "source": [
    "path = \"testImages/visualOdometryTestImages\"\n",
    "folderChoice = 2\n",
    "\n",
    "path = \"\".join([path, \"/\", str(folderChoice)])\n",
    "\n",
    "imageGlobL = sorted(glob.glob(\"\".join([path, \"/top_*\", \".png\"])))\n",
    "imageGlobR = sorted(glob.glob(\"\".join([path, \"/bottom_*\", \".png\"])))\n",
    "\n",
    "if not len(imageGlobL)==len(imageGlobR):\n",
    "    print(\"Images could not be matched\")\n",
    "\n",
    "print (\"Selections: 0-{}\".format(len(imageGlobL)-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15 15\n"
     ]
    }
   ],
   "source": [
    "# Top images\n",
    "imagesL = []\n",
    "for imageFile in imageGlobL:\n",
    "    imagesL.append(cv2.imread(imageFile))\n",
    "\n",
    "# Bottom images\n",
    "imagesR = []\n",
    "for imageFile in imageGlobR:\n",
    "    imagesR.append(cv2.imread(imageFile))\n",
    "\n",
    "print(len(imagesL), len(imagesR))"
   ]
  },
  {
   "source": [
    "### View image pair"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "558d592d76bd412c92a18d741d4adb2f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24a10055b38>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "selection = 0\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Image pair\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "                    np.hstack([imagesL[selection], imagesR[selection]]), \\\n",
    "                cv2.ROTATE_90_CLOCKWISE), \\\n",
    "            cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d96638404b344f48967a61a5fe2a64ab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24a0ffa5e48>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Sequential image pair\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "                    np.hstack([imagesL[selection], imagesL[selection+1]]), \\\n",
    "                cv2.ROTATE_90_CLOCKWISE), \\\n",
    "            cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "# Visual Odometry\n",
    "## Feature extraction\n",
    "### Initializing feature extractor"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features detected in frame 0: 500\nCoordinates of first keypoint in frame 0: (288.0, 130.0)\n"
     ]
    }
   ],
   "source": [
    "# Extraction function\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "def extractFeatures(image, orb=orb):\n",
    "    \"\"\"Find keypoints and descriptors for the image\"\"\"\n",
    "    keypoints = orb.detect(image, None)\n",
    "    keypoints, descriptors = orb.compute(image, keypoints)\n",
    "    \n",
    "    return keypoints, descriptors\n",
    "\n",
    "keypoints0, descriptors0 = extractFeatures(imagesL[selection])\n",
    "keypoints1, descriptors1 = extractFeatures(imagesL[selection+1])\n",
    "\n",
    "print(\"Number of features detected in frame {}: {}\"\\\n",
    "                                .format(selection, len(keypoints0)))\n",
    "print(\"Coordinates of first keypoint in frame {}: {}\"\\\n",
    "                                .format(selection, str(keypoints0[0].pt)))"
   ]
  },
  {
   "source": [
    "### Visualize extracted features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "463a32d6ea6e48a4bf2e0686482fec07"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e752aa0976824c70ba312694611e2794"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def visualizeFeatures(image, keypoints, flag):\n",
    "    \"\"\"Visualize extracted features in image\"\"\"\n",
    "    display = cv2.drawKeypoints(image, keypoints, None, flags=flag)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.imshow(cv2.cvtColor\\\n",
    "                (cv2.rotate(\\\n",
    "                    display, cv2.ROTATE_90_CLOCKWISE), \\\n",
    "                cv2.COLOR_BGR2RGB))\n",
    "\n",
    "visualizeFeatures(imagesL[selection], keypoints0, 4)\n",
    "visualizeFeatures(imagesL[selection], keypoints0, 2)"
   ]
  },
  {
   "source": [
    "## Initialize feature matcher"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features matched in frames 0 and 1: 100\n"
     ]
    }
   ],
   "source": [
    "bfMatcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "def matchFeatures(descriptors0, descriptors1, \\\n",
    "                                bfMatcher=bfMatcher, bestNMatches=100):\n",
    "    \"\"\"Match features from two images\"\"\"\n",
    "    match = bfMatcher.match(descriptors0, descriptors1)\n",
    "    match = sorted(match, key = lambda x:x.distance)\n",
    "\n",
    "    return match[:bestNMatches]\n",
    "\n",
    "match01 = matchFeatures(descriptors0, descriptors1, bfMatcher)\n",
    "\n",
    "print(\"Number of features matched in frames {} and {}: {}\"\\\n",
    "                        .format(selection, selection+1, len(match01)))"
   ]
  },
  {
   "source": [
    "### Visualize feature match"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0804cee26fb74548a80573b8292d9e52"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "def visualizeMatches(image0, keypoints0, image1, keypoints1, match):\n",
    "    imageMatches = cv2.drawMatches(image0, keypoints0, \\\n",
    "                            image1, keypoints1, match, None, flags=2)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "                    imageMatches, cv2.ROTATE_90_CLOCKWISE), \\\n",
    "                cv2.COLOR_BGR2RGB))\n",
    "\n",
    "visualizeMatches(imagesL[selection], keypoints0, \\\n",
    "                imagesL[selection+1], keypoints1, match01)"
   ]
  },
  {
   "source": [
    "## Trajectory Estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimated rotation:\n [[ 0.99924559  0.03707732 -0.01155537]\n [-0.03609422  0.99645086  0.07604537]\n [ 0.01433391 -0.07557091  0.9970374 ]]\nEstimated translation:\n [[ 0.04634577]\n [ 0.08936792]\n [-0.99491982]]\n"
     ]
    }
   ],
   "source": [
    "def estimateMotion(match, keypoints0, keypoints1, k=imageProcessor.cameraMatrixL):\n",
    "    \"\"\"Estimate camera motion from a pair of subsequent image frames\"\"\"\n",
    "    imagePoints0 = []\n",
    "    imagePoints1 = []\n",
    "\n",
    "    for m in match:\n",
    "        train_idx = m.trainIdx\n",
    "        query_idx = m.queryIdx\n",
    "\n",
    "        p1x, p1y = keypoints0[query_idx].pt \n",
    "        imagePoints0.append([p1x,p1y])\n",
    "\n",
    "        p2x,p2y = keypoints1[train_idx].pt \n",
    "        imagePoints1.append([p2x,p2y])\n",
    "    \n",
    "    E, mask = cv2.findEssentialMat(\\\n",
    "                    np.array(imagePoints0), np.array(imagePoints1), k, \\\n",
    "                    cv2.RANSAC, 0.999, 1.0) \n",
    "\n",
    "    retval, rmat, tvec, mask = cv2.recoverPose(E, np.array(imagePoints0), \\\n",
    "                    np.array(imagePoints1), k)\n",
    "\n",
    "    return rmat, tvec, imagePoints0, imagePoints1\n",
    "\n",
    "rmat, tvec, imagePoints0, imagePoints1 = estimateMotion(match01, \\\n",
    "                        keypoints0, keypoints1)\n",
    "\n",
    "print(\"Estimated rotation:\\n {0}\".format(rmat))\n",
    "print(\"Estimated translation:\\n {0}\".format(tvec))"
   ]
  },
  {
   "source": [
    "## Movement visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2caae45193c4c99bda18c9e1df7f644"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24a24824860>"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "def visualizeCameraMovement(image0, imagePoints0, \\\n",
    "                image1, imagePoints1, showImageAfterMove=False):\n",
    "    \"\"\"Visualize camera movement across frames\"\"\"\n",
    "    image0 = image0.copy()\n",
    "    image1 = image1.copy()\n",
    "\n",
    "    for i in range(0, len(imagePoints0)):\n",
    "        # Coordinates of a point on t frame\n",
    "        p1 = (int(imagePoints0[i][0]), int(imagePoints0[i][1]))\n",
    "        # Coordinates of the same point on t+1 frame\n",
    "        p2 = (int(imagePoints1[i][0]), int(imagePoints1[i][1]))\n",
    "\n",
    "        cv2.circle(image0, p1, 5, (0, 255, 0), 1)\n",
    "        cv2.arrowedLine(image0, p1, p2, (0, 255, 0), 1)\n",
    "        cv2.circle(image0, p2, 5, (255, 0, 0), 1)\n",
    "\n",
    "        if showImageAfterMove:\n",
    "            cv2.circle(image1, p2, 5, (255, 0, 0), 1)\n",
    "    \n",
    "    if showImageAfterMove: \n",
    "        return image1\n",
    "    else:\n",
    "        return image0\n",
    "\n",
    "imageMovementBefore = visualizeCameraMovement(imagesL[selection], \\\n",
    "                    imagePoints0, imagesL[selection+1], imagePoints1)\n",
    "\n",
    "imageMovementAfter = visualizeCameraMovement(imagesL[selection], \\\n",
    "                    imagePoints0, imagesL[selection+1], imagePoints1, \\\n",
    "                    showImageAfterMove=True)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "                np.hstack([imageMovementBefore, imageMovementAfter]), \\\n",
    "                    cv2.ROTATE_90_CLOCKWISE), \\\n",
    "                cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "# Voxel grid\n",
    "## Creating disparity map\n",
    "### Convert to grayscale and undistort"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "376b74e77e75492bbe670098c56f4e60"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71431704de9b461dabd145215538f180"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76420c9bd5ce46cd88a8db755e5a6a9b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24a249145c0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "imageProcessor.convertToGrayscale(imagesL[selection], imagesR[selection])\n",
    "imageProcessor.undistortRectifyRemap(imageProcessor.grayImageL, \\\n",
    "                                        imageProcessor.grayImageR)\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "fig.suptitle(\"left/right undistorted\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "            np.hstack([imageProcessor.undistortImageL, \\\n",
    "            imageProcessor.undistortImageR]), \\\n",
    "            cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "fig = plt.figure(figsize=(7,7))\n",
    "fig.suptitle(\"horizontal epipolar\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(\\\n",
    "        imageProcessor.drawHorEpipolarLines(\\\n",
    "        imageProcessor.undistortImageL, imageProcessor.undistortImageR), \\\n",
    "        cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))\n",
    "\n",
    "fig = plt.figure(figsize=(7,12))\n",
    "fig.suptitle(\"left/right undistorted\")\n",
    "plt.imshow(cv2.cvtColor(imageProcessor.drawVertEpipolarLines(\\\n",
    "        imageProcessor.undistortImageL, imageProcessor.undistortImageR), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### Compute disparity map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "minDisparity: 9.0\nmaxDisparity: 41.0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41721a0cce9f48a4af8d63a946008799"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24a1068c4e0>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "stereoMatcher.computeDisparity(\\\n",
    "                grayImageL=imageProcessor.undistortImageL, \\\n",
    "                grayImageR=imageProcessor.undistortImageR)\n",
    "\n",
    "stereoMatcher.clampDisparity()\n",
    "stereoMatcher.applyClosingFilter()\n",
    "\n",
    "print (\"minDisparity:\", stereoMatcher.disparityMapL.min())\n",
    "print (\"maxDisparity:\", stereoMatcher.disparityMapL.max())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cv2.rotate(stereoMatcher.disparityMapL, \\\n",
    "    cv2.ROTATE_90_CLOCKWISE))"
   ]
  },
  {
   "source": [
    "### Compute depth map (not necessary)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9.0\n41.0\n(640, 360)\n481.33795\n2192.7617\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7810eab0fb74cb18d898ca0acd2d744"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24a256390f0>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "focalLength = imageProcessor.projectionMatrixL[0][0] # changes with rectify?\n",
    "baseline = 32 # mm, measured irl\n",
    "\n",
    "stereoMatcher.disparityMapL[stereoMatcher.disparityMapL==0] = 0.9\n",
    "stereoMatcher.disparityMapL[stereoMatcher.disparityMapL==-1] = 0.9\n",
    "\n",
    "depthMap = np.empty_like(stereoMatcher.disparityMapL)\n",
    "depthMap = (focalLength*baseline)/stereoMatcher.disparityMapL[:]\n",
    "\n",
    "print (stereoMatcher.disparityMapL.min())\n",
    "print (stereoMatcher.disparityMapL.max())\n",
    "print (depthMap.shape)\n",
    "print (depthMap.min())\n",
    "print (depthMap.max())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cv2.rotate(depthMap, cv2.ROTATE_90_CLOCKWISE))"
   ]
  },
  {
   "source": [
    "## Compute voxel grid\n",
    "### Compute, filter, and rotate point cloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Points in unfiltered pointcloud: 9600\nPoints in filtered pointcloud: 6214\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0984e23dbf3f479dbb498464b8594f6d"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "pointSubsample = 24\n",
    "voxelSize = 100\n",
    "voxelStopFraction = 10\n",
    "occupancyThreshold = 10\n",
    "\n",
    "def generatePointCloud(disparityMapL, dispToDepthMatrix):\n",
    "    points = cv2.reprojectImageTo3D(\\\n",
    "            disparityMapL, \\\n",
    "            dispToDepthMatrix)\n",
    "\n",
    "    # Reshaping to a list of 3D coordinates\n",
    "    pointCloud = points.reshape(\\\n",
    "                (points.shape[0]*points.shape[1],3))[0::pointSubsample]\\\n",
    "                                                .astype(np.int16)\n",
    "\n",
    "    return pointCloud\n",
    "\n",
    "pointCloud = generatePointCloud(stereoMatcher.disparityMapL,\\\n",
    "                                imageProcessor.dispToDepthMatrix)\n",
    "\n",
    "print(\"Points in unfiltered pointcloud: {}\".format(pointCloud.shape[0]))\n",
    "\n",
    "def filterPointCloud(pointCloud):\n",
    "    # Filtering x values\n",
    "    pointCloud = pointCloud[np.logical_and(\\\n",
    "            pointCloud[:, 0]>pointCloud[:, 0].min(), \\\n",
    "            pointCloud[:, 0]<pointCloud[:, 0].max())]\n",
    "    # Filtering y values\n",
    "    pointCloud = pointCloud[np.logical_and(\\\n",
    "            pointCloud[:, 1]>pointCloud[:, 1].min(), \\\n",
    "            pointCloud[:, 1]<pointCloud[:, 1].max())]\n",
    "    # Filtering z values\n",
    "    pointCloud = pointCloud[np.logical_and(\\\n",
    "            pointCloud[:, 2]>pointCloud[:, 2].min(), \\\n",
    "            pointCloud[:, 2]<pointCloud[:, 2].max())]\n",
    "\n",
    "    return pointCloud\n",
    "\n",
    "pointCloud = filterPointCloud(pointCloud)\n",
    "\n",
    "print(\"Points in filtered pointcloud: {}\".format(pointCloud.shape[0]))\n",
    "\n",
    "redefineRotationMatrix = np.array([ [ 0,  0, -1],\n",
    "                                    [ 0,  1,  0],\n",
    "                                    [ 1,  0,  0] ])\n",
    "\n",
    "def rotateGrid(grid, rotationMatrix):\n",
    "    \"\"\"Rotate point cloud or voxel grid with given rotation matrix\"\"\"\n",
    "    grid = np.dot(grid[:], rotationMatrix)\n",
    "\n",
    "    return grid\n",
    "\n",
    "def plotGrid(grid, s):\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(111, projection = \"3d\")\n",
    "\n",
    "    ax.scatter(grid[:,0], grid[:,1], grid[:,2], s=s)\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_zlabel(\"$z$\")\n",
    "\n",
    "    # Camera axis begins at x=0 and looks to positive x\n",
    "    ax.set_xlim(0,2000)\n",
    "    ax.set_ylim(-1000,1000)\n",
    "    ax.set_zlim(-1000,1000)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plotGrid(rotateGrid(pointCloud, redefineRotationMatrix), 1)"
   ]
  },
  {
   "source": [
    "### Voxelize point cloud"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Voxels in grid: 56; 71 iterations\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6dc25c3c65f4835af3230d79e170afe"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "baseVoxelGrid = None\n",
    "\n",
    "def voxelizePointCloud(pointCloud, voxelSize, \\\n",
    "                        occupancyThreshold, voxelStopFraction):\n",
    "    \"\"\"Create voxelized representation of given point cloud\"\"\"\n",
    "    iterations = 0\n",
    "    newVoxelGrid = []\n",
    "    initialSize = pointCloud.shape[0]\n",
    "    remainingPoints = initialSize\n",
    "    samplingLimit = np.zeros_like(pointCloud[0])\n",
    "        \n",
    "    while remainingPoints>(initialSize/voxelStopFraction):\n",
    "\n",
    "        sampledPoint = pointCloud[np.random.randint(0,remainingPoints)]\n",
    "\n",
    "        for n in range(3):\n",
    "            samplingLimit[n]=\\\n",
    "                (sampledPoint[n]//voxelSize)*voxelSize\n",
    "\n",
    "        mask = np.ones(remainingPoints, dtype=bool)\n",
    "\n",
    "        for n in range(len(sampledPoint)):\n",
    "            mask = np.logical_and(mask, np.logical_and(\\\n",
    "                pointCloud[:,n]>=samplingLimit[n], \\\n",
    "                pointCloud[:,n]<samplingLimit[n]+voxelSize))\n",
    "\n",
    "        pointsInVoxel = pointCloud[mask]\n",
    "\n",
    "        if len(pointsInVoxel)>occupancyThreshold:\n",
    "            voxelMidpoint = samplingLimit+voxelSize/2\n",
    "            newVoxelGrid.append(voxelMidpoint)\n",
    "\n",
    "        pointCloud = pointCloud[np.invert(mask)]\n",
    "\n",
    "        iterations+=1\n",
    "\n",
    "        remainingPoints = pointCloud.shape[0]\n",
    "\n",
    "    newVoxelGrid = np.array(newVoxelGrid, dtype=np.int16)\n",
    "\n",
    "    return newVoxelGrid, iterations\n",
    "\n",
    "baseVoxelGrid, iterations = voxelizePointCloud(pointCloud, voxelSize, \\\n",
    "                        occupancyThreshold, voxelStopFraction)\n",
    "\n",
    "baseVoxelGrid = rotateGrid(baseVoxelGrid, redefineRotationMatrix)\n",
    "\n",
    "print(\"\".join([\"Voxels in grid: {}; {} iterations\"]).format(\\\n",
    "                baseVoxelGrid.shape[0], iterations))\n",
    "\n",
    "plotGrid(baseVoxelGrid, voxelSize)"
   ]
  },
  {
   "source": [
    "## Refine voxel grid\n",
    "### Fetching camera fov"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Horizontal: 24.915932084055157\nVertical: 14.867033874651561\n"
     ]
    }
   ],
   "source": [
    "# Horizontal field of view (degrees)\n",
    "fovH = ((imageProcessor.fovYL+imageProcessor.fovYR)/4)*np.pi/180\n",
    "fovH -= fovH/8\n",
    "print(\"Horizontal:\", fovH*180/np.pi)\n",
    "\n",
    "# Vertical field of view (degrees)\n",
    "fovV = ((imageProcessor.fovXL+imageProcessor.fovXR)/4)*np.pi/180\n",
    "fovV -= fovV/8\n",
    "print(\"Vertical:\", fovV*180/np.pi)"
   ]
  },
  {
   "source": [
    "### Get voxels near camera"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnVoxelsInRange(voxelGrid, cameraPosition, distance=1500):\n",
    "    \"\"\"Return voxels that are within given distance from cameraPosition \n",
    "    in voxelGrid, along with yaw and distance\"\"\"\n",
    "    translatedVoxels = voxelGrid-cameraPosition\n",
    "    distanceToVoxels = np.linalg.norm(translatedVoxels, axis=1)\n",
    "\n",
    "    voxelsInRange = voxelGrid[distanceToVoxels<=distance]\n",
    "    translatedVoxelsInRange = voxelsInRange - cameraPosition\n",
    "\n",
    "    distanceToVoxelsInRange = distanceToVoxels[distanceToVoxels<=distance]\n",
    "    yawToVoxelsInRange = np.arctan2(translatedVoxelsInRange[:,1], \\\n",
    "                                        translatedVoxelsInRange[:,0])\n",
    "\n",
    "    return voxelsInRange, yawToVoxelsInRange, distanceToVoxelsInRange"
   ]
  },
  {
   "source": [
    "### Get yaw of camera"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCameraYawRange(rotationMatrix, currentRotationMatrix=None):\n",
    "    \"\"\"Return camera yaw due to rotationMatrix on currentRotationMatrix\"\"\"\n",
    "    cameraDirectionVector = np.array([0,0,100])\n",
    "\n",
    "    cameraDirectionVector = np.dot(cameraDirectionVector, rotationMatrix)\n",
    "\n",
    "    cameraYaw = np.arctan2(cameraDirectionVector[1], cameraDirectionVector[0])\n",
    "\n",
    "    cameraYawRange = np.array([cameraYaw+fovH, cameraYaw-fovH])\n",
    "    # Wrapping around values at -180, 180 degrees\n",
    "    for n in range(len(cameraYawRange)):\n",
    "        if cameraYawRange[n]>np.pi:\n",
    "            cameraYawRange[n] -= 2*np.pi\n",
    "        if cameraYawRange[n]<=-np.pi:\n",
    "            cameraYawRange[n] += 2*np.pi\n",
    "    cameraYawRange = np.sort(cameraYawRange)[::-1]\n",
    "\n",
    "    return cameraYawRange, cameraYaw"
   ]
  },
  {
   "source": [
    "### Remove voxels in view of the camera from the base grid"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in1d_dot_approach(A,B):\n",
    "    \"\"\"Returns the first array with elements from the second array removed\"\"\"\n",
    "    cumdims = (np.maximum(A.max(),B.max())+1)**np.arange(B.shape[1])\n",
    "    return A[~np.in1d(A.dot(cumdims),B.dot(cumdims))]\n",
    "\n",
    "def removeVoxelsInView(baseVoxelGrid, voxelsInRange, \\\n",
    "                                    yawToVoxelsInRange, cameraYawRange):\n",
    "    \"\"\"Remove voxels that are in range, and in view of the camera\"\"\"\n",
    "    if cameraYawRange[0]>np.pi/2 and cameraYawRange[1]<-np.pi/2:\n",
    "        voxelsToRemove = voxelsInRange[np.logical_and(\\\n",
    "            yawToVoxelsInRange[:]>cameraYawRange[0], \\\n",
    "            yawToVoxelsInRange[:]<cameraYawRange[1]\n",
    "            )]\n",
    "    else:\n",
    "        voxelsToRemove = voxelsInRange[np.logical_and(\\\n",
    "            yawToVoxelsInRange[:]<cameraYawRange[0], \\\n",
    "            yawToVoxelsInRange[:]>cameraYawRange[1]\n",
    "            )]\n",
    "\n",
    "    if voxelsToRemove.shape[0]!=0:\n",
    "        voxelRemovedGrid = in1d_dot_approach(baseVoxelGrid, voxelsToRemove)\n",
    "        return voxelRemovedGrid\n",
    "\n",
    "    else:\n",
    "        return baseVoxelGrid"
   ]
  },
  {
   "source": [
    "### Combine unique voxels from base and new voxel grids"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineVoxelGrids(baseVoxelGrid, newVoxelGrid):\n",
    "    \"\"\"Combine unique voxels from given voxel grids\"\"\"\n",
    "    return np.unique(np.vstack([baseVoxelGrid, newVoxelGrid]), axis=0)"
   ]
  },
  {
   "source": [
    "## Combined voxelizing function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNewVoxelGrid(disparityMapL, dispToDepthMatrix, rotationMatrix, \\\n",
    "        cameraPosition, voxelSize, occupancyThreshold, voxelStopFraction):\n",
    "    \"\"\"Generate a new voxel grid from the given disparity map\"\"\"\n",
    "    # Compute point cloud\n",
    "    pointCloud = generatePointCloud(disparityMapL, dispToDepthMatrix)\n",
    "    # Filter point cloud\n",
    "    pointCloud = filterPointCloud(pointCloud)\n",
    "    # Compute new voxel grid\n",
    "    newVoxelGrid, iterations = voxelizePointCloud(pointCloud, voxelSize, \\\n",
    "                        occupancyThreshold, voxelStopFraction)\n",
    "    # Rotate voxel grid\n",
    "    newVoxelGrid = rotateGrid(newVoxelGrid, rotationMatrix)\n",
    "    # Translate voxel grid\n",
    "    newVoxelGrid += cameraPosition\n",
    "\n",
    "    return newVoxelGrid, iterations"
   ]
  },
  {
   "source": [
    "### Testing combined function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38aee064bf4f4c57bdbe1b82e60ed885"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "cameraPosition = [0,0,0]\n",
    "newVoxelGrid, iterations = getNewVoxelGrid(stereoMatcher.disparityMapL,\\\n",
    "                    imageProcessor.dispToDepthMatrix, redefineRotationMatrix, \\\n",
    "                    cameraPosition, voxelSize, occupancyThreshold, \\\n",
    "                    voxelStopFraction)\n",
    "\n",
    "plotGrid(newVoxelGrid, voxelSize)"
   ]
  },
  {
   "source": [
    "# Combined pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deques support thread-safe, memory efficient appends and pops from \n",
    "# either side with approximately the same O(1) performance in either \n",
    "# direction\n",
    "# Pronounced 'deck'; 'double ended queue'\n",
    "keypoints = deque(maxlen=2)\n",
    "descriptors = deque(maxlen=2)\n",
    "\n",
    "# Initializing empty voxel grid\n",
    "baseVoxelGrid = None\n",
    "\n",
    "# Initializing camera position, trajectory, and rotation\n",
    "cameraPosition = np.array([0,0,0])\n",
    "rotation = deque([redefineRotationMatrix.copy()])\n",
    "trajectory = deque([np.array([0, 0, 0])])\n",
    "\n",
    "R = redefineRotationMatrix.copy()\n",
    "T = np.zeros([3, 1])\n",
    "RT = np.hstack([R, T])\n",
    "RT = np.vstack([RT, np.zeros([1, 4])])\n",
    "RT[-1, -1] = 1\n",
    "\n",
    "for imageL, imageR in zip(imagesL, imagesR):\n",
    "    # Extract features and append to deques\n",
    "    imageKeypoints, imageDescriptors = extractFeatures(imageL)\n",
    "    keypoints.append(imageKeypoints)\n",
    "    descriptors.append(imageDescriptors)\n",
    "\n",
    "    # Estimate motion after the first frame/iteration\n",
    "    if len(keypoints)==2:\n",
    "        # Match features\n",
    "        match = matchFeatures(descriptors[0], descriptors[1])\n",
    "\n",
    "        # Estimate motion between the images\n",
    "        rmat, tvec, imagePoints0, imagePoints1 = estimateMotion(match, \\\n",
    "                        keypoints[0], keypoints[1])\n",
    "\n",
    "        # Estimate cumulative trajectory and rotation\n",
    "        rt_mtx = np.hstack([rmat, tvec])\n",
    "        rt_mtx = np.vstack([rt_mtx, np.zeros([1, 4])])\n",
    "        rt_mtx[-1, -1] = 1\n",
    "\n",
    "        rt_mtx_inv = np.linalg.inv(rt_mtx)\n",
    "        \n",
    "        RT = np.dot(RT, rt_mtx_inv)\n",
    "\n",
    "        newTrajectory = RT[:3, 3]\n",
    "        newRotation = RT[:3, :3]\n",
    "\n",
    "        # Append updated trajectory and rotation matrices\n",
    "        trajectory.appendleft(newTrajectory*100)\n",
    "        rotation.appendleft(newRotation)\n",
    "\n",
    "    # Convert image to grayscale and undistort\n",
    "    imageProcessor.convertToGrayscale(imageL, imageR)\n",
    "    imageProcessor.undistortRectifyRemap(imageProcessor.grayImageL, \\\n",
    "                                        imageProcessor.grayImageR)\n",
    "\n",
    "    # Compute disparity\n",
    "    stereoMatcher.computeDisparity(\\\n",
    "                    grayImageL=imageProcessor.undistortImageL, \\\n",
    "                    grayImageR=imageProcessor.undistortImageR)\n",
    "    stereoMatcher.clampDisparity()\n",
    "    stereoMatcher.applyClosingFilter()\n",
    "\n",
    "    # Compute new voxel grid\n",
    "    newVoxelGrid, iterations = getNewVoxelGrid(stereoMatcher.disparityMapL,\\\n",
    "                    imageProcessor.dispToDepthMatrix, rotation[0], \\\n",
    "                    trajectory[0], voxelSize, occupancyThreshold, \\\n",
    "                    voxelStopFraction)\n",
    "    \n",
    "    # For all iterations except the first\n",
    "    if baseVoxelGrid is not None:\n",
    "        # Compute camera view range\n",
    "        cameraYawRange, cameraYaw = getCameraYawRange(rotation[0])\n",
    "\n",
    "        # Get voxels in range of the camera\n",
    "        voxelsInRange, yawToVoxelsInRange, distanceToVoxelsInRange = \\\n",
    "                returnVoxelsInRange(baseVoxelGrid, trajectory[0])\n",
    "\n",
    "        # Remove voxels from base grid in range and in view of the camera\n",
    "        voxelRemovedGrid = removeVoxelsInView(baseVoxelGrid, voxelsInRange, \\\n",
    "                                    yawToVoxelsInRange, cameraYawRange)\n",
    "\n",
    "        # Combine the new and base voxel grids\n",
    "        baseVoxelGrid = combineVoxelGrids(voxelRemovedGrid, newVoxelGrid)\n",
    "\n",
    "    # For the first iteration\n",
    "    else:\n",
    "        baseVoxelGrid = newVoxelGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4466b2bfccac426baccbd4a8433d586c"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7,7))\n",
    "ax = fig.add_subplot(111, projection = \"3d\")\n",
    "\n",
    "# Plot accumulated voxel grid\n",
    "ax.scatter(baseVoxelGrid[:,0], \\\n",
    "            baseVoxelGrid[:,1], \\\n",
    "            baseVoxelGrid[:,2], \\\n",
    "            s=voxelSize)\n",
    "\n",
    "# Plot trajectory\n",
    "trajectoryPlot = np.array(trajectory)\n",
    "ax.plot(trajectoryPlot[:,0], \\\n",
    "        trajectoryPlot[:,1], \\\n",
    "        trajectoryPlot[:,2])\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel(\"$x$\")\n",
    "ax.set_ylabel(\"$y$\")\n",
    "ax.set_zlabel(\"$z$\")\n",
    "\n",
    "# Setting axis limits\n",
    "# Camera axis begins at x=0 and looks to positive x\n",
    "ax.set_xlim(0,2000)\n",
    "ax.set_ylim(-1000,1000)\n",
    "ax.set_zlim(-1000,1000)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}