{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd01ca5eb8a425185c4a9b7e413fe88a69d4f417f1bd440f1366b1dc8976c436489",
   "display_name": "Python 3.8.8 64-bit ('envbot': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Visual odometry pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import requirements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib ipympl\n",
    "\n",
    "from ImageProcessor import ImageProcessor\n"
   ]
  },
  {
   "source": [
    "### Load images and depth map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selections: 0-1\n"
     ]
    }
   ],
   "source": [
    "folderChoice = 0\n",
    "path = \"\".join([\"testImages/visualOdometryTestImages/\", str(folderChoice)])\n",
    "\n",
    "# Load set of top images\n",
    "imageGlobT = sorted(glob.glob(\"\".join([path, \"/top_*\", \".png\"])))\n",
    "\n",
    "# Load set of bottom images\n",
    "imageGlobB = sorted(glob.glob(\"\".join([path, \"/bottom_*\", \".png\"])))\n",
    "\n",
    "# Load depth map\n",
    "imageGlobD = sorted(glob.glob(\"\".join([path, \"/topDepth_*\", \".png\"])))\n",
    "\n",
    "if not (len(imageGlobT)==len(imageGlobB) and \\\n",
    "                len(imageGlobB)==len(imageGlobD)):\n",
    "    print(\"Images could not be matched\")\n",
    "\n",
    "print (\"Selections: 0-{}\".format(len(imageGlobT)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "viewSelection = 0\n",
    "\n",
    "# Top images\n",
    "imageT0 = cv2.imread(imageGlobT[viewSelection])\n",
    "imageT1 = cv2.imread(imageGlobT[viewSelection+1])\n",
    "\n",
    "# Bottom images\n",
    "imageB0 = cv2.imread(imageGlobB[viewSelection])\n",
    "imageB1 = cv2.imread(imageGlobB[viewSelection+1])\n",
    "\n",
    "# Depth maps; -1 flag to load them as is\n",
    "depth0 = cv2.imread(imageGlobD[viewSelection], -1)\n",
    "depth1 = cv2.imread(imageGlobD[viewSelection+1], -1)"
   ]
  },
  {
   "source": [
    "### View top camera images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c143e294fcf4694bf22e98a52c7cd23"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ea80309f70>"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.suptitle(\"Top images\")\n",
    "plt.imshow(cv2.cvtColor(np.hstack([imageT0, imageT1]), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### View bottom camera images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4ce015c40174957b4f087b180998e9b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ea8ba5f6a0>"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.suptitle(\"Bottom images\")\n",
    "plt.imshow(cv2.cvtColor(np.hstack([imageB0, imageB1]), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### View depth images\n",
    "Depth images correspond to top camera"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cc9be06a46d8463f93d4271654c0ceb1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ea8bb3a6d0>"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.suptitle(\"Depth maps\")\n",
    "plt.imshow(np.hstack([depth0, depth1]))"
   ]
  },
  {
   "source": [
    "### Feature Extraction\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "orb = cv2.ORB_create()\n",
    "#def extract_features(img):\n",
    "def extract_features(img1,img2):\n",
    "    keyp1 = orb.detect(img1,None)\n",
    "    keyp1,des1 = orb.compute(img1, keyp1)\n",
    "    keyp2 = orb.detect(img2,None)\n",
    "    keyp2,des2 = orb.compute(img2, keyp2)\n",
    "    \n",
    "    img1 = cv2.drawKeypoints(img1, keyp1, None, color=(0,255,0),flags=0)\n",
    "    img2 = cv2.drawKeypoints(img2,keyp2,None,color=(0,255,0),flags=0) \n",
    "    plt.figure()\n",
    "    # plt.imshow(img2), \n",
    "    plt.imshow(cv2.cvtColor(np.hstack([img1, img2]), cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    return (keyp1,des1),(keyp2,des2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b0673fdde82740008a9960ad8b3300f9"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "orb1,orb2=extract_features(imageT0,imageT1)\n"
   ]
  },
  {
   "source": [
    "Feature matching\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10f5fb998c774f92a9b3aa7d1f62b7a6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x1ea8c1edb20>, None)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "keyp1 = orb.detect(imageT0,None)\n",
    "keyp1,des1 = orb.compute(imageT0, keyp1)\n",
    "keyp2 = orb.detect(imageT1,None)\n",
    "keyp2,des2 = orb.compute(imageT1, keyp2)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(des1,des2)\n",
    "matches = sorted(matches, key = lambda x:x.distance)\n",
    "img3 = cv2.drawMatches(imageT0,keyp1,imageT1,keyp2,matches[:10],None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "plt.figure()\n",
    "plt.imshow(img3),plt.show()\n"
   ]
  },
  {
   "source": [
    "Number of Matched features\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features matched in frames {0} and {1}: {2} 250\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features matched in frames {0} and {1}: {2}\",len(matches))"
   ]
  },
  {
   "source": [
    "Trajectory Estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_motion():\n",
    "    imageProcessor=ImageProcessor()\n",
    "    imageProcessor.loadMonoCalibration()\n",
    "    k=imageProcessor.cameraMatrixL\n",
    "    \n",
    "    rmat=np.eye(3)\n",
    "    tvec=np.zeros((3,1))\n",
    "    image1_points = []\n",
    "    image2_points = []\n",
    "\n",
    "    for m in matches:\n",
    "        train_idx = m.trainIdx\n",
    "        query_idx = m.queryIdx\n",
    "\n",
    "        p1x,p1y = keyp1[query_idx].pt \n",
    "        image1_points.append([p1x,p1y])\n",
    "\n",
    "        p2x,p2y = keyp2[train_idx].pt \n",
    "        image2_points.append([p2x,p2y])\n",
    "    E, mask = cv2.findEssentialMat(np.array(image1_points), np.array(image2_points), k) \n",
    "    retval,rmat,tvec,mask=cv2.recoverPose(E,np.array(image1_points),np.array(image2_points),k)\n",
    "    return rmat,tvec,image1_points,image2_points      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading from data/monoCalibration.json\n",
      "Loaded mono calibration\n",
      "Estimated rotation:\n",
      " [[ 0.99445956 -0.03551991  0.09893699]\n",
      " [ 0.04459262  0.99484699 -0.09105477]\n",
      " [-0.09519291  0.09496215  0.99091902]]\n",
      "Estimated translation:\n",
      " [[-0.65096869]\n",
      " [ 0.63698372]\n",
      " [ 0.41290617]]\n"
     ]
    }
   ],
   "source": [
    "i = 30\n",
    "match = matches[i]\n",
    "rmat, tvec, image1_points, image2_points = estimate_motion()\n",
    "\n",
    "print(\"Estimated rotation:\\n {0}\".format(rmat))\n",
    "print(\"Estimated translation:\\n {0}\".format(tvec))"
   ]
  }
 ]
}