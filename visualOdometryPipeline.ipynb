{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd01b74fdc6a0069e63d7c35520416cfe241b2e1d296eedc56ee9c4fe2929446925",
   "display_name": "Python 3.6.8  ('bot': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "1b74fdc6a0069e63d7c35520416cfe241b2e1d296eedc56ee9c4fe2929446925"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Visual odometry pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import requirements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ImageProcessor import ImageProcessor\n",
    "from StereoMatcher import StereoMatcher\n",
    "from VoxelGrid import VoxelGrid\n",
    "from helperScripts.TimeKeeper import TimeKeeper\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "source": [
    "### Load images and depth map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selections: 0-6\n"
     ]
    }
   ],
   "source": [
    "folderChoice = 1\n",
    "path = \"\".join([\"testImages/visualOdometryTestImages/\", str(folderChoice)])\n",
    "\n",
    "# Load set of top images\n",
    "imageGlobT = sorted(glob.glob(\"\".join([path, \"/top_*\", \".png\"])))\n",
    "\n",
    "# Load set of bottom images\n",
    "imageGlobB = sorted(glob.glob(\"\".join([path, \"/bottom_*\", \".png\"])))\n",
    "\n",
    "# Load depth map\n",
    "imageGlobD = sorted(glob.glob(\"\".join([path, \"/topDepth_*\", \".png\"])))\n",
    "\n",
    "if not (len(imageGlobT)==len(imageGlobB) and \\\n",
    "                len(imageGlobB)==len(imageGlobD)):\n",
    "    print(\"Images could not be matched\")\n",
    "\n",
    "print (\"Selections: 0-{}\".format(len(imageGlobT)-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8 8 8\n"
     ]
    }
   ],
   "source": [
    "# Top images\n",
    "imagesT = []\n",
    "for imageFile in imageGlobT:\n",
    "    imagesT.append(cv2.imread(imageFile))\n",
    "\n",
    "# Bottom images\n",
    "imagesB = []\n",
    "for imageFile in imageGlobB:\n",
    "    imagesB.append(cv2.imread(imageFile))\n",
    "\n",
    "# Depth maps; -1 flag to load them as is\n",
    "depthMaps = []\n",
    "for imageFile in imageGlobD:\n",
    "    depthMaps.append(cv2.imread(imageFile, -1))\n",
    "\n",
    "print(len(imagesT), len(imagesB), len(depthMaps))"
   ]
  },
  {
   "source": [
    "### View top camera images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4ca94794b41480ca4b6d3089f82bac1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23808931898>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "selection = 0\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Top images\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(np.hstack([imagesT[selection], imagesT[selection+1]]), cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### View bottom camera images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3649c03afa843d8adcc404b6af901b4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23808851b70>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Bottom images\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(np.hstack([imagesB[selection], imagesB[selection+1]]), cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### View depth images\n",
    "Depth images correspond to top camera"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6f86fdb64dbc4a31926efbd7d7a99a86"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x238088d8080>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Depth maps\")\n",
    "plt.imshow(cv2.rotate(np.hstack([depthMaps[selection], depthMaps[selection+1]]), cv2.ROTATE_90_CLOCKWISE))"
   ]
  },
  {
   "source": [
    "### Depth map units and dimensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(640, 360)\n1025 mm\n"
     ]
    }
   ],
   "source": [
    "print(depthMaps[0].shape)\n",
    "print(depthMaps[0][int(depthMaps[0].shape[0]/2), \\\n",
    "                    int(depthMaps[0].shape[1]/2)], \"mm\")"
   ]
  },
  {
   "source": [
    "### Instantiating TimeKeeper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeKeeper = TimeKeeper()"
   ]
  },
  {
   "source": [
    "### Loading camera calibration matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading from data/monoCalibration.json\nLoaded mono calibration\nReading from data/parametersSGBM.json\n[[ 0  0 -1]\n [ 0  1  0]\n [ 1  0  0]]\n[[592.63974229   0.         188.47568825]\n [  0.         592.98181459 312.47351841]\n [  0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "imageProcessor = ImageProcessor()\n",
    "imageProcessor.loadMonoCalibration()\n",
    "\n",
    "stereoMatcher = StereoMatcher(imageProcessor=imageProcessor)\n",
    "\n",
    "voxelGrid = VoxelGrid(stereoMatcher=stereoMatcher, imageProcessor=imageProcessor)\n",
    "\n",
    "baseRotation = voxelGrid.redefineRotationMatrix\n",
    "k=imageProcessor.cameraMatrixL\n",
    "\n",
    "print(baseRotation)\n",
    "print(k)"
   ]
  },
  {
   "source": [
    "### Feature Extraction\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(orb, image):\n",
    "    \"\"\"Find keypoints and descriptors for the image\"\"\"\n",
    "    keypoints = orb.detect(image, None)\n",
    "    keypoints, descriptors = orb.compute(image, keypoints)\n",
    "    \n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features detected in frame 0: 500\nCoordinates of first keypoint in frame 0: (183.0, 415.0)\nCompleted in 0.134836 seconds\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "keypoints, descriptors = extractFeatures(orb, imagesT[selection])\n",
    "\n",
    "print(\"Number of features detected in frame {}: {}\"\\\n",
    "                                .format(selection, len(keypoints)))\n",
    "print(\"Coordinates of first keypoint in frame {}: {}\"\\\n",
    "                                .format(selection, str(keypoints[0].pt)))\n",
    "\n",
    "timeKeeper.printPerfCounter()"
   ]
  },
  {
   "source": [
    "### Visualize features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeFeatures(image, keypoints, flag):\n",
    "    \"\"\"Visualize extracted features in image\"\"\"\n",
    "    display = cv2.drawKeypoints(image, keypoints, None, flags=flag)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(cv2.cvtColor(cv2.rotate(display, cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "134e5462823440bd9e6a8d55657b8960"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeFeatures(imagesT[selection], keypoints, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dac61e0166d24cd990eddf31ef64375f"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeFeatures(imagesT[selection], keypoints, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAllFeatures(images, extractFeatures, orb):\n",
    "    \"\"\"Find keypoints and descriptors for each image in folder\"\"\"\n",
    "    allKeypoints = []\n",
    "    allDescriptors = []\n",
    "    \n",
    "    for image in images:\n",
    "        keypoints, descriptors = extractFeatures(orb, image)\n",
    "        allKeypoints.append(keypoints)\n",
    "        allDescriptors.append(descriptors)\n",
    "    \n",
    "    return allKeypoints, allDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8 8\nNumber of features detected in frame 0: 500\nCoordinates of the first keypoint in frame 0: (183.0, 415.0)\n\nCompleted in 0.063373 seconds\nAverage time for each extraction: 0.00792\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "allKeypoints, allDescriptors = extractAllFeatures(imagesT, extractFeatures, orb)\n",
    "\n",
    "print(len(allKeypoints), len(allDescriptors))\n",
    "\n",
    "print(\"Number of features detected in frame {}: {}\"\\\n",
    "                            .format(selection, len(allKeypoints[selection])))\n",
    "print(\"Coordinates of the first keypoint in frame {}: {}\\n\"\\\n",
    "                            .format(selection, str(allKeypoints[selection][0].pt)))\n",
    "\n",
    "timeKeeper.printPerfCounter()\n",
    "print(\"Average time for each extraction: {:.5f}\".format(timeKeeper.getElapsedTime()/len(imagesT)))"
   ]
  },
  {
   "source": [
    "### Feature matching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchFeatures(bfMatcher, descriptors0, descriptors1, bestNMatches):\n",
    "    \"\"\"Match features from two images\"\"\"\n",
    "    match = bfMatcher.match(descriptors0, descriptors1)\n",
    "    match = sorted(match, key = lambda x:x.distance)\n",
    "\n",
    "    return match[:bestNMatches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features matched in frames 0 and 1: 50\nCompleted in 0.001671 seconds\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "bfMatcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "bestNMatches = 50\n",
    "\n",
    "match = matchFeatures(bfMatcher, allDescriptors[selection], \\\n",
    "                    allDescriptors[selection+1], bestNMatches)\n",
    "\n",
    "print(\"Number of features matched in frames {} and {}: {}\"\\\n",
    "                        .format(selection, selection+1, len(match)))\n",
    "\n",
    "timeKeeper.printPerfCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeMatches(image0, keypoints0, image1, keypoints1, match):\n",
    "    imageMatches = cv2.drawMatches(image0, keypoints0, \\\n",
    "                            image1, keypoints1, match, None, flags=2)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(cv2.cvtColor(imageMatches, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9d34417d1a64adf94787bd96b40423f"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeMatches(imagesT[selection], allKeypoints[selection], \\\n",
    "                imagesT[selection+1], allKeypoints[selection+1], match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchAllFeatures(allDescriptors, matchFeatures, bfMatcher, bestNMatches):\n",
    "    \"\"\"Match features for each subsequent image pair in the dataset\"\"\"\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(allDescriptors)-1):\n",
    "        descriptor1 = allDescriptors[i]\n",
    "        descriptor2 = allDescriptors[i+1]\n",
    "\n",
    "        match = matchFeatures(bfMatcher, descriptor1, descriptor2, bestNMatches)\n",
    "\n",
    "        matches.append(match)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features matched in frames 0 and 1: 50\nCompleted in 0.008213 seconds\nAverage time for each frame pair match: 0.00103\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "matches = matchAllFeatures(allDescriptors, matchFeatures, bfMatcher, bestNMatches)\n",
    "\n",
    "print(\"Number of features matched in frames {} and {}: {}\"\\\n",
    "            .format(selection, selection+1, len(matches[selection])))\n",
    "\n",
    "timeKeeper.printPerfCounter()\n",
    "print(\"Average time for each frame pair match: {:.5f}\".format(timeKeeper.getElapsedTime()/len(imagesT)))"
   ]
  },
  {
   "source": [
    "### Trajectory Estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateMotion(match, keypoints0, keypoints1, k, depthMap=None, usePnP=False):\n",
    "    \"\"\"Estimate camera motion from a pair of subsequent image frames\"\"\"\n",
    "    rmat=np.eye(3)\n",
    "    tvec=np.zeros((3,1))\n",
    "    imagePoints0 = []\n",
    "    imagePoints1 = []\n",
    "\n",
    "    for m in match:\n",
    "        train_idx = m.trainIdx\n",
    "        query_idx = m.queryIdx\n",
    "\n",
    "        p1x, p1y = keypoints0[query_idx].pt \n",
    "        imagePoints0.append([p1x,p1y])\n",
    "\n",
    "        p2x,p2y = keypoints1[train_idx].pt \n",
    "        imagePoints1.append([p2x,p2y])\n",
    "    \n",
    "    E, mask = cv2.findEssentialMat(\\\n",
    "                    np.array(imagePoints0), np.array(imagePoints1), k, \\\n",
    "                    cv2.RANSAC, 0.999, 1.0) \n",
    "\n",
    "    retval, rmat, tvec, mask = cv2.recoverPose(E, np.array(imagePoints0), \\\n",
    "                    np.array(imagePoints1), k)\n",
    "\n",
    "    return rmat, tvec, imagePoints0, imagePoints1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimated rotation:\n [[ 0.9996192  -0.01071935 -0.02542735]\n [ 0.01100595  0.99987718  0.01115802]\n [ 0.02530462 -0.01143362  0.9996144 ]]\nEstimated translation:\n [[ 0.30938726]\n [-0.23494812]\n [-0.92145478]]\nCompleted in 0.003020 seconds\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "match = matches[selection]\n",
    "rmat, tvec, imagePoints0, imagePoints1 = estimateMotion(\\\n",
    "                        match, allKeypoints[selection], \\\n",
    "                        allKeypoints[selection+1], k, \\\n",
    "                        depthMap=depthMaps[selection])\n",
    "\n",
    "print(\"Estimated rotation:\\n {0}\".format(rmat))\n",
    "print(\"Estimated translation:\\n {0}\".format(tvec))\n",
    "\n",
    "timeKeeper.printPerfCounter()"
   ]
  },
  {
   "source": [
    "### Camera movement visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeCameraMovement(image0, imagePoints0, \\\n",
    "                image1, imagePoints1, showImageAfterMove=False):\n",
    "    \"\"\"Visualize camera movement across frames\"\"\"\n",
    "    image0 = image0.copy()\n",
    "    image1 = image1.copy()\n",
    "\n",
    "    for i in range(0, len(imagePoints0)):\n",
    "        # Coordinates of a point on t frame\n",
    "        p1 = (int(imagePoints0[i][0]), int(imagePoints0[i][1]))\n",
    "        # Coordinates of the same point on t+1 frame\n",
    "        p2 = (int(imagePoints1[i][0]), int(imagePoints1[i][1]))\n",
    "\n",
    "        cv2.circle(image0, p1, 5, (0, 255, 0), 1)\n",
    "        cv2.arrowedLine(image0, p1, p2, (0, 255, 0), 1)\n",
    "        cv2.circle(image0, p2, 5, (255, 0, 0), 1)\n",
    "\n",
    "        if showImageAfterMove:\n",
    "            cv2.circle(image1, p2, 5, (255, 0, 0), 1)\n",
    "    \n",
    "    if showImageAfterMove: \n",
    "        return image1\n",
    "    else:\n",
    "        return image0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "485cca0d07d6482b9c71dda5a243c9d5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23809b88748>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "imageMovementBefore = visualizeCameraMovement(imagesT[selection], \\\n",
    "                    imagePoints0, imagesT[selection+1], imagePoints1)\n",
    "\n",
    "imageMovementAfter = visualizeCameraMovement(imagesT[selection], \\\n",
    "                    imagePoints0, imagesT[selection+1], imagePoints1, \\\n",
    "                    showImageAfterMove=True)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cv2.cvtColor(np.hstack([imageMovementBefore, imageMovementAfter]), \\\n",
    "                                                        cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### Camera Trajectory Estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateTrajectory(estimateMotion, matches, allKeypoints, k, \\\n",
    "                                                baseRotation, depthMaps):\n",
    "    \"\"\"Estimate complete camera trajectory from subsequent image pairs\"\"\"\n",
    "    trajectory = [np.array([0, 0, 0])]\n",
    "    rotation = [baseRotation]\n",
    "    #rotation = [np.eye(3)]\n",
    "\n",
    "    #R = np.diag([1,1,1])\n",
    "    R = baseRotation.copy().T\n",
    "    T = np.zeros([3, 1])\n",
    "    RT = np.hstack([R, T])\n",
    "    RT = np.vstack([RT, np.zeros([1, 4])])\n",
    "    RT[-1, -1] = 1\n",
    "\n",
    "    for i in range(len(matches)):     \n",
    "        match = matches[i]\n",
    "        keypoints0 = allKeypoints[i]\n",
    "        keypoints1 = allKeypoints[i+1]\n",
    "        depth = depthMaps[i]\n",
    "\n",
    "        rmat, tvec, imagePoints0, imagePoints1 = estimateMotion(\\\n",
    "                                match, keypoints0, keypoints1, k, depthMap=depth)\n",
    "\n",
    "        rt_mtx = np.hstack([rmat, tvec])\n",
    "        rt_mtx = np.vstack([rt_mtx, np.zeros([1, 4])])\n",
    "        rt_mtx[-1, -1] = 1\n",
    "\n",
    "        rt_mtx_inv = np.linalg.inv(rt_mtx)\n",
    "        \n",
    "        RT = np.dot(RT, rt_mtx_inv)\n",
    "\n",
    "        newTrajectory = RT[:3, 3]\n",
    "        newRotation = RT[:3, :3]\n",
    "\n",
    "        trajectory.append(newTrajectory*100)\n",
    "        rotation.append(newRotation)\n",
    "\n",
    "    trajectory = np.array(trajectory)\n",
    "    rotation = np.array(rotation)\n",
    "\n",
    "    return trajectory, rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Camera location in point 0 is: \n [0. 0. 0.]\n\nCamera location in point 1 is: \n [93.15879167 22.77001281 28.3366556 ]\n\nLength of trajectory: 8\nCompleted in 0.016792 seconds\nAverage time for trajectory segment: 0.00210\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "trajectory, rotation = estimateTrajectory(\\\n",
    "                        estimateMotion, matches, allKeypoints, k, \\\n",
    "                        baseRotation, depthMaps=depthMaps)\n",
    "\n",
    "print(\"Camera location in point {} is: \\n {}\\n\"\\\n",
    "                        .format(selection, trajectory[selection,:]))\n",
    "print(\"Camera location in point {} is: \\n {}\\n\"\\\n",
    "                        .format(selection+1, trajectory[selection+1,:]))\n",
    "print(\"Length of trajectory: {}\".format(trajectory.shape[0]))\n",
    "\n",
    "timeKeeper.printPerfCounter()\n",
    "print(\"Average time for trajectory segment: {:.5f}\"\\\n",
    "                        .format(timeKeeper.getElapsedTime()/len(imagesT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trajectory at each step:\n [[  0.           0.           0.        ]\n [ 93.15879167  22.77001281  28.3366556 ]\n [192.11090022  32.25405547  39.22394861]\n [290.56734095  31.18262541  21.75450912]\n [381.65482048  67.18863733   1.58923754]\n [354.99285251  52.41346215  96.83016184]\n [256.69168825  35.87255234 104.78499736]\n [157.38220023  24.14115945 104.78185707]]\n\nRotation at each step:\n [[[ 0.          0.         -1.        ]\n  [ 0.          1.          0.        ]\n  [ 1.          0.          0.        ]]\n\n [[-0.02542735  0.01115802  0.9996144 ]\n  [-0.01071935  0.99987718 -0.01143362]\n  [-0.9996192  -0.01100595 -0.02530462]]\n\n [[-0.06144454  0.00909787  0.99806903]\n  [-0.02152959  0.99971372 -0.0104383 ]\n  [-0.99787827 -0.0221294  -0.06123107]]\n\n [[-0.07278604 -0.01460331  0.99724066]\n  [ 0.01469408  0.99976857  0.01571281]\n  [-0.99723933  0.0157972  -0.07255461]]\n\n [[-0.10504747  0.01583001  0.99434121]\n  [ 0.02107338  0.99968422 -0.01368876]\n  [-0.9942439   0.01951616 -0.10534789]]\n\n [[-0.0310653   0.01551255  0.99939697]\n  [-0.01327009  0.99978503 -0.01593106]\n  [-0.99942926 -0.01375699 -0.03085277]]\n\n [[ 0.03032328  0.02221541  0.99929324]\n  [-0.0118176   0.99969107 -0.02186565]\n  [-0.99947028 -0.01114621  0.03057644]]\n\n [[ 0.10176942  0.03109804  0.99432183]\n  [-0.00620747  0.99951167 -0.03062502]\n  [-0.99478865 -0.00305554  0.10191276]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Trajectory at each step:\\n\", trajectory)\n",
    "print(\"\\nRotation at each step:\\n\", rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTrajectory(trajectory):\n",
    "    \"\"\"Show a 3D plot of the trajectory\"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    ax.plot(trajectory[:,0], \\\n",
    "            trajectory[:,1], \\\n",
    "            trajectory[:,2])\n",
    "\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_zlabel(\"$z$\")\n",
    "\n",
    "    ax.set_xlim(-1000,1000)\n",
    "    ax.set_ylim(-1000,1000)\n",
    "    ax.set_zlim(-1000,1000)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a5a6b3d8f0f4fb886a5cc1d52c6da7d"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeTrajectory(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}