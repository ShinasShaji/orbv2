{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd01b74fdc6a0069e63d7c35520416cfe241b2e1d296eedc56ee9c4fe2929446925",
   "display_name": "Python 3.6.8 64-bit ('bot': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Visual odometry pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import requirements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ImageProcessor import ImageProcessor\n",
    "from helperScripts.TimeKeeper import TimeKeeper\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "source": [
    "### Load images and depth map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selections: 0-1\n"
     ]
    }
   ],
   "source": [
    "folderChoice = 0\n",
    "path = \"\".join([\"testImages/visualOdometryTestImages/\", str(folderChoice)])\n",
    "\n",
    "# Load set of top images\n",
    "imageGlobT = sorted(glob.glob(\"\".join([path, \"/top_*\", \".png\"])))\n",
    "\n",
    "# Load set of bottom images\n",
    "imageGlobB = sorted(glob.glob(\"\".join([path, \"/bottom_*\", \".png\"])))\n",
    "\n",
    "# Load depth map\n",
    "imageGlobD = sorted(glob.glob(\"\".join([path, \"/topDepth_*\", \".png\"])))\n",
    "\n",
    "if not (len(imageGlobT)==len(imageGlobB) and \\\n",
    "                len(imageGlobB)==len(imageGlobD)):\n",
    "    print(\"Images could not be matched\")\n",
    "\n",
    "print (\"Selections: 0-{}\".format(len(imageGlobT)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n2\n2\n"
     ]
    }
   ],
   "source": [
    "# Top images\n",
    "imagesT = []\n",
    "for imageFile in imageGlobT:\n",
    "    imagesT.append(cv2.imread(imageFile))\n",
    "\n",
    "# Bottom images\n",
    "imagesB = []\n",
    "for imageFile in imageGlobB:\n",
    "    imagesB.append(cv2.imread(imageFile))\n",
    "\n",
    "# Depth maps; -1 flag to load them as is\n",
    "depthMaps = []\n",
    "for imageFile in imageGlobD:\n",
    "    depthMaps.append(cv2.imread(imageFile, -1))\n",
    "\n",
    "print(len(imagesT))\n",
    "print(len(imagesB))\n",
    "print(len(depthMaps))"
   ]
  },
  {
   "source": [
    "### View top camera images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c13dba20ccd4413ac053f47d01349a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2440c834358>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.suptitle(\"Top images\")\n",
    "plt.imshow(cv2.cvtColor(np.hstack(imagesT), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### View bottom camera images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c38a1148872043a1b11f4ae7214661f3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2440cbb4588>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.suptitle(\"Bottom images\")\n",
    "plt.imshow(cv2.cvtColor(np.hstack(imagesB), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### View depth images\n",
    "Depth images correspond to top camera"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "90d783e66ebe41e08cf72208e5cf133f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2440d274b70>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.suptitle(\"Depth maps\")\n",
    "plt.imshow(np.hstack(depthMaps))"
   ]
  },
  {
   "source": [
    "### Depth map units and dimensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(640, 360)\n750 mm\n"
     ]
    }
   ],
   "source": [
    "print(depthMaps[0].shape)\n",
    "print(depthMaps[0][int(depthMaps[0].shape[0]/2), \\\n",
    "                    int(depthMaps[0].shape[1]/2)], \"mm\")"
   ]
  },
  {
   "source": [
    "### Instantiating TimeKeeper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timekeeper = TimeKeeper()"
   ]
  },
  {
   "source": [
    "### Loading camera calibration matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading from data/monoCalibration.json\nLoaded mono calibration\n[[592.63974229   0.         188.47568825]\n [  0.         592.98181459 312.47351841]\n [  0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "imageProcessor=ImageProcessor()\n",
    "imageProcessor.loadMonoCalibration()\n",
    "k=imageProcessor.cameraMatrixL\n",
    "print(k)"
   ]
  },
  {
   "source": [
    "### Feature Extraction\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(orb, image):\n",
    "    \"\"\"Find keypoins and descriptors for the image\"\"\"\n",
    "    keypoints = orb.detect(image, None)\n",
    "    keypoints, descriptors = orb.compute(image, keypoints)\n",
    "    \n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features detected in frame 0: 500\nCoordinates of first keypoint in frame 0: (299.0, 596.0)\n"
     ]
    }
   ],
   "source": [
    "selection = 0\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "keypoints, descriptors = extractFeatures(orb, imagesT[selection])\n",
    "\n",
    "print(\"Number of features detected in frame {}: {}\"\\\n",
    "                                .format(selection, len(keypoints)))\n",
    "print(\"Coordinates of first keypoint in frame {}: {}\"\\\n",
    "                                .format(selection, str(keypoints[0].pt)))\n"
   ]
  },
  {
   "source": [
    "### Visualize features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeFeatures(image, keypoints, flag):\n",
    "    \"\"\"Visualize extracted features in image\"\"\"\n",
    "    display = cv2.drawKeypoints(image, keypoints, None, flags=flag)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(cv2.cvtColor(display, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4b2ac7e438ef4a01b096b96a3c005817"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeFeatures(imagesT[selection], keypoints, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAllFeatures(images, extractFeatures, orb):\n",
    "    \"\"\"Find keypoints and descriptors for each image in folder\"\"\"\n",
    "    allKeypoints = []\n",
    "    allDescriptors = []\n",
    "    \n",
    "    for image in images:\n",
    "        keypoints, descriptors = extractFeatures(orb, image)\n",
    "        allKeypoints.append(keypoints)\n",
    "        allDescriptors.append(descriptors)\n",
    "    \n",
    "    return allKeypoints, allDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2 2\nNumber of features detected in frame 0: 500\nCoordinates of the first keypoint in frame 0: (299.0, 596.0)\n\n"
     ]
    }
   ],
   "source": [
    "allKeypoints, allDescriptors = extractAllFeatures(imagesT, extractFeatures, orb)\n",
    "\n",
    "print(len(allKeypoints), len(allDescriptors))\n",
    "\n",
    "print(\"Number of features detected in frame {}: {}\"\\\n",
    "                            .format(selection, len(allKeypoints[selection])))\n",
    "print(\"Coordinates of the first keypoint in frame {}: {}\\n\"\\\n",
    "                            .format(selection, str(allKeypoints[selection][0].pt)))"
   ]
  },
  {
   "source": [
    "### Feature matching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchFeatures(bfMatcher, descriptors0, descriptors1, bestNMatches):\n",
    "    \"\"\"Match features from two images\"\"\"\n",
    "    match = bfMatcher.match(descriptors0, descriptors1)\n",
    "    match = sorted(match, key = lambda x:x.distance)\n",
    "\n",
    "    return match[:bestNMatches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features matched in frames 0 and 1: 50\n"
     ]
    }
   ],
   "source": [
    "bfMatcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "bestNMatches = 50\n",
    "\n",
    "match = matchFeatures(bfMatcher, allDescriptors[selection], \\\n",
    "                    allDescriptors[selection+1], bestNMatches)\n",
    "\n",
    "print(\"Number of features matched in frames {} and {}: {}\"\\\n",
    "                        .format(selection, selection+1, len(match)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeMatches(image0, keypoints0, image1, keypoints1, match):\n",
    "    imageMatches = cv2.drawMatches(image0, keypoints0, \\\n",
    "                            image1, keypoints1, match, None, flags=2)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(cv2.cvtColor(imageMatches, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f362e367ae244172a7a5aeb9b3a483e7"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeMatches(imagesT[selection], allKeypoints[selection], \\\n",
    "                imagesT[selection+1], allKeypoints[selection+1], match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchAllFeatures(allDescriptors, matchFeatures, bfMatcher, bestNMatches):\n",
    "    \"\"\"Match features for each subsequent image pair in the dataset\"\"\"\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(allDescriptors)-1):\n",
    "        descriptor1 = allDescriptors[i]\n",
    "        descriptor2 = allDescriptors[i+1]\n",
    "\n",
    "        match = matchFeatures(bfMatcher, descriptor1, descriptor2, bestNMatches)\n",
    "\n",
    "        matches.append(match)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features matched in frames 0 and 1: 50\n"
     ]
    }
   ],
   "source": [
    "matches = matchAllFeatures(allDescriptors, matchFeatures, bfMatcher, bestNMatches)\n",
    "\n",
    "print(\"Number of features matched in frames {} and {}: {}\"\\\n",
    "            .format(selection, selection+1, len(matches[selection])))"
   ]
  },
  {
   "source": [
    "### Trajectory Estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_motion(match, keyp1, keyp2, k, depth0=None):\n",
    "    rmat=np.eye(3)\n",
    "    tvec=np.zeros((3,1))\n",
    "    image1_points = []\n",
    "    image2_points = []\n",
    "\n",
    "    for m in match:\n",
    "        train_idx = m.trainIdx\n",
    "        query_idx = m.queryIdx\n",
    "\n",
    "        p1x,p1y = keyp1[query_idx].pt \n",
    "        image1_points.append([p1x,p1y])\n",
    "\n",
    "        p2x,p2y = keyp2[train_idx].pt \n",
    "        image2_points.append([p2x,p2y])\n",
    "\n",
    "    E, mask = cv2.findEssentialMat(np.array(image1_points), np.array(image2_points), k) \n",
    "\n",
    "    retval,rmat,tvec,mask=cv2.recoverPose(E,np.array(image1_points),np.array(image2_points),k)\n",
    "\n",
    "    return rmat,tvec,image1_points,image2_points      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimated rotation:\n [[ 0.99445956 -0.03551991  0.09893699]\n [ 0.04459262  0.99484699 -0.09105477]\n [-0.09519291  0.09496215  0.99091902]]\nEstimated translation:\n [[-0.65096869]\n [ 0.63698372]\n [ 0.41290617]]\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "match = matches[i]\n",
    "rmat, tvec, image1_points, image2_points = estimate_motion(match, keyp1, keyp2, k, depth0=depth0)\n",
    "\n",
    "print(\"Estimated rotation:\\n {0}\".format(rmat))\n",
    "print(\"Estimated translation:\\n {0}\".format(tvec))"
   ]
  },
  {
   "source": [
    "Camera Movement Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_camera_movement(image1, image1_points, image2, image2_points, is_show_img_after_move=False):\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(image1_points)):\n",
    "        # Coordinates of a point on t frame\n",
    "        p1 = (int(image1_points[i][0]), int(image1_points[i][1]))\n",
    "        # Coordinates of the same point on t+1 frame\n",
    "        p2 = (int(image2_points[i][0]), int(image2_points[i][1]))\n",
    "\n",
    "        cv2.circle(image1, p1, 5, (0, 255, 0), 1)\n",
    "        cv2.arrowedLine(image1, p1, p2, (0, 255, 0), 1)\n",
    "        cv2.circle(image1, p2, 5, (255, 0, 0), 1)\n",
    "\n",
    "        if is_show_img_after_move:\n",
    "            cv2.circle(image2, p2, 5, (255, 0, 0), 1)\n",
    "    \n",
    "    if is_show_img_after_move: \n",
    "        return imageT1\n",
    "    else:\n",
    "        return imageT0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b71cf15e25c844f78aebe465893cdf20"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19cb2f449b0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "image_move = visualize_camera_movement(imageT0, image1_points, imageT1, image2_points)\n",
    "plt.figure(figsize=(7,7),dpi=100)\n",
    "plt.imshow(image_move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d9d845d126c478fa521c15bb1df9b25"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19cb3028630>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "image_move = visualize_camera_movement(imageT0, image1_points, imageT1, image2_points, is_show_img_after_move=True)\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(image_move)"
   ]
  },
  {
   "source": [
    "### Camera Trajectory Estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_trajectory(estimate_motion, matches):\n",
    "    trajectory = [np.array([0, 0, 0])]\n",
    "    R = np.diag([1,1,1])\n",
    "    T = np.zeros([3, 1])\n",
    "    RT = np.hstack([R, T])\n",
    "    RT = np.vstack([RT, np.zeros([1, 4])])\n",
    "    RT[-1, -1] = 1\n",
    "    for i in range(len(matches)):     \n",
    "        match = matches[i]\n",
    "\n",
    "        rmat, tvec, image1_points, image2_points = estimate_motion(match, keyp1, keyp2, k)\n",
    "        rt_mtx = np.hstack([rmat, tvec])\n",
    "        rt_mtx = np.vstack([rt_mtx, np.zeros([1, 4])])\n",
    "        rt_mtx[-1, -1] = 1\n",
    "\n",
    "        rt_mtx_inv = np.linalg.inv(rt_mtx)\n",
    "        \n",
    "        RT = np.dot(RT, rt_mtx_inv)\n",
    "        new_trajectory = RT[:3, 3]\n",
    "        trajectory.append(new_trajectory)\n",
    "\n",
    "    trajectory = np.array(trajectory).T   \n",
    "    return trajectory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading from data/monoCalibration.json\nLoaded mono calibration\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "estimate_motion() missing 4 required positional arguments: 'match', 'keyp1', 'keyp2', and 'k'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-9bd280963107>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrajectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_trajectory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Camera location in point {0} is: \\n {1}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Length of trajectory: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrajectory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-0aa9947fb7c7>\u001b[0m in \u001b[0;36mestimate_trajectory\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mrmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtvec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage1_points\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage2_points\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_motion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mrt_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtvec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mrt_mtx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrt_mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: estimate_motion() missing 4 required positional arguments: 'match', 'keyp1', 'keyp2', and 'k'"
     ]
    }
   ],
   "source": [
    "trajectory = estimate_trajectory(estimate_motion, matches, kp_list, k, depth_maps=[])\n",
    "\n",
    "i=0\n",
    "print(\"Camera location in point {0} is: \\n {1}\\n\".format(i, trajectory[:, [i]]))\n",
    "print(\"Length of trajectory: {0}\".format(trajectory.shape[0]))"
   ]
  }
 ]
}