{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd034d9fd4794b4aa62af7c694b698b8481ba4eb70d629f53257a855dbffddd8207",
   "display_name": "Python 3.7.3 32-bit ('bot': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Visual odometry pipeline"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import requirements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from ImageProcessor import ImageProcessor\n",
    "from helperScripts.TimeKeeper import TimeKeeper\n",
    "\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "source": [
    "### Load images and depth map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selections: 0-6\n"
     ]
    }
   ],
   "source": [
    "folderChoice = 1\n",
    "path = \"\".join([\"testImages/visualOdometryTestImages/\", str(folderChoice)])\n",
    "\n",
    "# Load set of top images\n",
    "imageGlobT = sorted(glob.glob(\"\".join([path, \"/top_*\", \".png\"])))\n",
    "\n",
    "# Load set of bottom images\n",
    "imageGlobB = sorted(glob.glob(\"\".join([path, \"/bottom_*\", \".png\"])))\n",
    "\n",
    "# Load depth map\n",
    "imageGlobD = sorted(glob.glob(\"\".join([path, \"/topDepth_*\", \".png\"])))\n",
    "\n",
    "if not (len(imageGlobT)==len(imageGlobB) and \\\n",
    "                len(imageGlobB)==len(imageGlobD)):\n",
    "    print(\"Images could not be matched\")\n",
    "\n",
    "print (\"Selections: 0-{}\".format(len(imageGlobT)-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8 8 8\n"
     ]
    }
   ],
   "source": [
    "# Top images\n",
    "imagesT = []\n",
    "for imageFile in imageGlobT:\n",
    "    imagesT.append(cv2.imread(imageFile))\n",
    "\n",
    "# Bottom images\n",
    "imagesB = []\n",
    "for imageFile in imageGlobB:\n",
    "    imagesB.append(cv2.imread(imageFile))\n",
    "\n",
    "# Depth maps; -1 flag to load them as is\n",
    "depthMaps = []\n",
    "for imageFile in imageGlobD:\n",
    "    depthMaps.append(cv2.imread(imageFile, -1))\n",
    "\n",
    "print(len(imagesT), len(imagesB), len(depthMaps))"
   ]
  },
  {
   "source": [
    "### View top camera images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9228957c6c345618d1c9a59409a384e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xa55232d0>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "selection = 0\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Top images\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(np.hstack([imagesT[selection], imagesT[selection+1]]), cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### View bottom camera images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68aeda49e1b44356adc8654cbccd209e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xa49363b0>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Bottom images\")\n",
    "plt.imshow(cv2.cvtColor(cv2.rotate(np.hstack([imagesB[selection], imagesB[selection+1]]), cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### View depth images\n",
    "Depth images correspond to top camera"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2bc5c8b7c5bc4209b92b22f528728ab7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xa48fed90>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.suptitle(\"Depth maps\")\n",
    "plt.imshow(cv2.rotate(np.hstack([depthMaps[selection], depthMaps[selection+1]]), cv2.ROTATE_90_CLOCKWISE))"
   ]
  },
  {
   "source": [
    "### Depth map units and dimensions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(640, 360)\n1025 mm\n"
     ]
    }
   ],
   "source": [
    "print(depthMaps[0].shape)\n",
    "print(depthMaps[0][int(depthMaps[0].shape[0]/2), \\\n",
    "                    int(depthMaps[0].shape[1]/2)], \"mm\")"
   ]
  },
  {
   "source": [
    "### Instantiating TimeKeeper"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeKeeper = TimeKeeper()"
   ]
  },
  {
   "source": [
    "### Loading camera calibration matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reading from data/monoCalibration.json\nLoaded mono calibration\n[[592.63974229   0.         188.47568825]\n [  0.         592.98181459 312.47351841]\n [  0.           0.           1.        ]]\n"
     ]
    }
   ],
   "source": [
    "imageProcessor=ImageProcessor()\n",
    "imageProcessor.loadMonoCalibration()\n",
    "k=imageProcessor.cameraMatrixL\n",
    "print(k)"
   ]
  },
  {
   "source": [
    "### Feature Extraction\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(orb, image):\n",
    "    \"\"\"Find keypoints and descriptors for the image\"\"\"\n",
    "    keypoints = orb.detect(image, None)\n",
    "    keypoints, descriptors = orb.compute(image, keypoints)\n",
    "    \n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features detected in frame 0: 500\nCoordinates of first keypoint in frame 0: (172.0, 487.0)\nCompleted in 0.129870 seconds\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "keypoints, descriptors = extractFeatures(orb, imagesT[selection])\n",
    "\n",
    "print(\"Number of features detected in frame {}: {}\"\\\n",
    "                                .format(selection, len(keypoints)))\n",
    "print(\"Coordinates of first keypoint in frame {}: {}\"\\\n",
    "                                .format(selection, str(keypoints[0].pt)))\n",
    "\n",
    "timeKeeper.printPerfCounter()"
   ]
  },
  {
   "source": [
    "### Visualize features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeFeatures(image, keypoints, flag):\n",
    "    \"\"\"Visualize extracted features in image\"\"\"\n",
    "    display = cv2.drawKeypoints(image, keypoints, None, flags=flag)\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(cv2.cvtColor(cv2.rotate(display, cv2.ROTATE_90_CLOCKWISE), cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "011b8206cb0348b6b3b60213474f0b41"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeFeatures(imagesT[selection], keypoints, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a05cd0c8d4cc4d26a65791d4965c80e7"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeFeatures(imagesT[selection], keypoints, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractAllFeatures(images, extractFeatures, orb):\n",
    "    \"\"\"Find keypoints and descriptors for each image in folder\"\"\"\n",
    "    allKeypoints = []\n",
    "    allDescriptors = []\n",
    "    \n",
    "    for image in images:\n",
    "        keypoints, descriptors = extractFeatures(orb, image)\n",
    "        allKeypoints.append(keypoints)\n",
    "        allDescriptors.append(descriptors)\n",
    "    \n",
    "    return allKeypoints, allDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8 8\nNumber of features detected in frame 0: 500\nCoordinates of the first keypoint in frame 0: (172.0, 487.0)\n\nCompleted in 0.659287 seconds\nAverage time for each extraction: 0.08241\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "allKeypoints, allDescriptors = extractAllFeatures(imagesT, extractFeatures, orb)\n",
    "\n",
    "print(len(allKeypoints), len(allDescriptors))\n",
    "\n",
    "print(\"Number of features detected in frame {}: {}\"\\\n",
    "                            .format(selection, len(allKeypoints[selection])))\n",
    "print(\"Coordinates of the first keypoint in frame {}: {}\\n\"\\\n",
    "                            .format(selection, str(allKeypoints[selection][0].pt)))\n",
    "\n",
    "timeKeeper.printPerfCounter()\n",
    "print(\"Average time for each extraction: {:.5f}\".format(timeKeeper.getElapsedTime()/len(imagesT)))"
   ]
  },
  {
   "source": [
    "### Feature matching"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchFeatures(bfMatcher, descriptors0, descriptors1, bestNMatches):\n",
    "    \"\"\"Match features from two images\"\"\"\n",
    "    match = bfMatcher.match(descriptors0, descriptors1)\n",
    "    match = sorted(match, key = lambda x:x.distance)\n",
    "\n",
    "    return match[:bestNMatches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features matched in frames 0 and 1: 50\nCompleted in 0.024168 seconds\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "bfMatcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "bestNMatches = 50\n",
    "\n",
    "match = matchFeatures(bfMatcher, allDescriptors[selection], \\\n",
    "                    allDescriptors[selection+1], bestNMatches)\n",
    "\n",
    "print(\"Number of features matched in frames {} and {}: {}\"\\\n",
    "                        .format(selection, selection+1, len(match)))\n",
    "\n",
    "timeKeeper.printPerfCounter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeMatches(image0, keypoints0, image1, keypoints1, match):\n",
    "    imageMatches = cv2.drawMatches(image0, keypoints0, \\\n",
    "                            image1, keypoints1, match, None, flags=2)\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(cv2.cvtColor(imageMatches, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8283efc20874bc0a008e9684e826151"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeMatches(imagesT[selection], allKeypoints[selection], \\\n",
    "                imagesT[selection+1], allKeypoints[selection+1], match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matchAllFeatures(allDescriptors, matchFeatures, bfMatcher, bestNMatches):\n",
    "    \"\"\"Match features for each subsequent image pair in the dataset\"\"\"\n",
    "    matches = []\n",
    "\n",
    "    for i in range(len(allDescriptors)-1):\n",
    "        descriptor1 = allDescriptors[i]\n",
    "        descriptor2 = allDescriptors[i+1]\n",
    "\n",
    "        match = matchFeatures(bfMatcher, descriptor1, descriptor2, bestNMatches)\n",
    "\n",
    "        matches.append(match)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features matched in frames 0 and 1: 50\nCompleted in 0.220910 seconds\nAverage time for each frame pair match: 0.02761\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "matches = matchAllFeatures(allDescriptors, matchFeatures, bfMatcher, bestNMatches)\n",
    "\n",
    "print(\"Number of features matched in frames {} and {}: {}\"\\\n",
    "            .format(selection, selection+1, len(matches[selection])))\n",
    "\n",
    "timeKeeper.printPerfCounter()\n",
    "print(\"Average time for each frame pair match: {:.5f}\".format(timeKeeper.getElapsedTime()/len(imagesT)))"
   ]
  },
  {
   "source": [
    "### Trajectory Estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateMotion(match, keypoints0, keypoints1, k, depthMap=None, usePnP=False):\n",
    "    \"\"\"Estimate camera motion from a pair of subsequent image frames\"\"\"\n",
    "    rmat=np.eye(3)\n",
    "    tvec=np.zeros((3,1))\n",
    "    imagePoints0 = []\n",
    "    imagePoints1 = []\n",
    "\n",
    "    if usePnP:\n",
    "        for m in match:\n",
    "            train_idx = m.trainIdx\n",
    "            query_idx = m.queryIdx\n",
    "\n",
    "            p1x, p1y = keypoints0[query_idx].pt \n",
    "            imagePoints0.append([p1x,p1y])\n",
    "\n",
    "            p2x,p2y = keypoints1[train_idx].pt \n",
    "            imagePoints1.append([p2x,p2y])\n",
    "\n",
    "    else:\n",
    "        for m in match:\n",
    "            train_idx = m.trainIdx\n",
    "            query_idx = m.queryIdx\n",
    "\n",
    "            p1x, p1y = keypoints0[query_idx].pt \n",
    "            imagePoints0.append([p1x,p1y])\n",
    "\n",
    "            p2x,p2y = keypoints1[train_idx].pt \n",
    "            imagePoints1.append([p2x,p2y])\n",
    "    \n",
    "        E, mask = cv2.findEssentialMat(\\\n",
    "                        np.array(imagePoints0), np.array(imagePoints1), k) \n",
    "\n",
    "    retval, rmat, tvec, mask = cv2.recoverPose(\\\n",
    "                        E, np.array(imagePoints0), np.array(imagePoints1), k)\n",
    "\n",
    "    return rmat, tvec, imagePoints0, imagePoints1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Estimated rotation:\n [[ 0.99975855 -0.0091354  -0.01998482]\n [ 0.00934511  0.99990198  0.01042551]\n [ 0.01988762 -0.01060976  0.99974593]]\nEstimated translation:\n [[ 0.23575014]\n [-0.23090275]\n [-0.943984  ]]\nCompleted in 0.057344 seconds\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "match = matches[selection]\n",
    "rmat, tvec, imagePoints0, imagePoints1 = estimateMotion(\\\n",
    "                        match, allKeypoints[selection], \\\n",
    "                        allKeypoints[selection+1], k, \\\n",
    "                        depthMap=depthMaps[selection])\n",
    "\n",
    "print(\"Estimated rotation:\\n {0}\".format(rmat))\n",
    "print(\"Estimated translation:\\n {0}\".format(tvec))\n",
    "\n",
    "timeKeeper.printPerfCounter()"
   ]
  },
  {
   "source": [
    "### Camera movement visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeCameraMovement(image0, imagePoints0, \\\n",
    "                image1, imagePoints1, showImageAfterMove=False):\n",
    "    \"\"\"Visualize camera movement across frames\"\"\"\n",
    "    image0 = image0.copy()\n",
    "    image1 = image1.copy()\n",
    "\n",
    "    for i in range(0, len(imagePoints0)):\n",
    "        # Coordinates of a point on t frame\n",
    "        p1 = (int(imagePoints0[i][0]), int(imagePoints0[i][1]))\n",
    "        # Coordinates of the same point on t+1 frame\n",
    "        p2 = (int(imagePoints1[i][0]), int(imagePoints1[i][1]))\n",
    "\n",
    "        cv2.circle(image0, p1, 5, (0, 255, 0), 1)\n",
    "        cv2.arrowedLine(image0, p1, p2, (0, 255, 0), 1)\n",
    "        cv2.circle(image0, p2, 5, (255, 0, 0), 1)\n",
    "\n",
    "        if showImageAfterMove:\n",
    "            cv2.circle(image1, p2, 5, (255, 0, 0), 1)\n",
    "    \n",
    "    if showImageAfterMove: \n",
    "        return image1\n",
    "    else:\n",
    "        return image0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "323bedf6835647f18d0e76681e61f136"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x9fca9670>"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "imageMovementBefore = visualizeCameraMovement(imagesT[selection], \\\n",
    "                    imagePoints0, imagesT[selection+1], imagePoints1)\n",
    "\n",
    "imageMovementAfter = visualizeCameraMovement(imagesT[selection], \\\n",
    "                    imagePoints0, imagesT[selection+1], imagePoints1, \\\n",
    "                    showImageAfterMove=True)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.imshow(cv2.cvtColor(np.hstack([imageMovementBefore, imageMovementAfter]), \\\n",
    "                                                        cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "source": [
    "### Camera Trajectory Estimation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateTrajectory(estimateMotion, matches, allKeypoints, k, depthMaps):\n",
    "    \"\"\"Estimate complete camera trajectory from subsequent image pairs\"\"\"\n",
    "    trajectory = [np.array([0, 0, 0])]\n",
    "    rotation = [np.eye(3)]\n",
    "\n",
    "    R = np.diag([1,1,1])\n",
    "    T = np.zeros([3, 1])\n",
    "    RT = np.hstack([R, T])\n",
    "    RT = np.vstack([RT, np.zeros([1, 4])])\n",
    "    RT[-1, -1] = 1\n",
    "\n",
    "    for i in range(len(matches)):     \n",
    "        match = matches[i]\n",
    "        keypoints0 = allKeypoints[i]\n",
    "        keypoints1 = allKeypoints[i+1]\n",
    "        depth = depthMaps[i]\n",
    "\n",
    "        rmat, tvec, imagePoints0, imagePoints1 = estimateMotion(\\\n",
    "                                match, keypoints0, keypoints1, k, depthMap=depth)\n",
    "\n",
    "        rt_mtx = np.hstack([rmat, tvec])\n",
    "        rt_mtx = np.vstack([rt_mtx, np.zeros([1, 4])])\n",
    "        rt_mtx[-1, -1] = 1\n",
    "\n",
    "        rt_mtx_inv = np.linalg.inv(rt_mtx)\n",
    "        \n",
    "        RT = np.dot(RT, rt_mtx_inv)\n",
    "\n",
    "        newTrajectory = RT[:3, 3]\n",
    "        newRotation = RT[:3, :3]\n",
    "\n",
    "        trajectory.append(newTrajectory)\n",
    "        rotation.append(newRotation)\n",
    "\n",
    "    trajectory = np.array(trajectory)\n",
    "    rotation = np.array(rotation)\n",
    "\n",
    "    return trajectory, rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Camera location in point 0 is: \n [0. 0. 0.]\n\nCamera location in point 1 is: \n [-0.21476182  0.22301835  0.95086286]\n\nLength of trajectory: 8\nCompleted in 0.116272 seconds\nAverage time for trajectory segment: 0.01453\n"
     ]
    }
   ],
   "source": [
    "timeKeeper.startPerfCounter()\n",
    "\n",
    "trajectory, rotation = estimateTrajectory(\\\n",
    "                        estimateMotion, matches, allKeypoints, k, depthMaps=depthMaps)\n",
    "\n",
    "print(\"Camera location in point {} is: \\n {}\\n\"\\\n",
    "                        .format(selection, trajectory[selection,:]))\n",
    "print(\"Camera location in point {} is: \\n {}\\n\"\\\n",
    "                        .format(selection+1, trajectory[selection+1,:]))\n",
    "print(\"Length of trajectory: {}\".format(trajectory.shape[0]))\n",
    "\n",
    "timeKeeper.printPerfCounter()\n",
    "print(\"Average time for trajectory segment: {:.5f}\".format(timeKeeper.getElapsedTime()/len(imagesT)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trajectory at each step:\n [[ 0.          0.          0.        ]\n [-0.21476182  0.22301835  0.95086286]\n [-0.29580899  0.27216776  1.94636057]\n [-0.75290545  1.05924822  2.36056725]\n [-0.88664823  0.52813602  3.19724681]\n [-1.15004807  0.5209491   2.23258684]\n [-1.33015293  0.45920958  1.2508789 ]\n [-2.23363338  0.86767565  1.1209615 ]]\n\nRotation at each step:\n [[[ 1.          0.          0.        ]\n  [ 0.          1.          0.        ]\n  [ 0.          0.          1.        ]]\n\n [[ 0.99975855  0.00934511  0.01988762]\n  [-0.0091354   0.99990198 -0.01060976]\n  [-0.01998482  0.01042551  0.99974593]]\n\n [[ 0.9982431   0.00683647  0.05885548]\n  [-0.00647609  0.99995911 -0.00631175]\n  [-0.05889623  0.00591951  0.99824656]]\n\n [[ 0.98331246  0.01998327  0.18082389]\n  [ 0.0089414   0.98743905 -0.15774716]\n  [-0.18170487  0.15673157  0.97078244]]\n\n [[ 0.96694928  0.01800513  0.25433228]\n  [ 0.0105814   0.99381033 -0.11058513]\n  [-0.25474914  0.10962141  0.96077366]]\n\n [[ 0.9899997   0.02395681  0.13902037]\n  [-0.00724073  0.99280503 -0.11952299]\n  [-0.14088351  0.11732111  0.98305015]]\n\n [[ 0.99677499  0.01862764  0.07805527]\n  [-0.00867273  0.9919943  -0.12598447]\n  [-0.07977717  0.12490122  0.98895667]]\n\n [[ 0.99596816  0.04281718  0.07882961]\n  [-0.02901472  0.98526231 -0.16857142]\n  [-0.0848856   0.16560454  0.98253222]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Trajectory at each step:\\n\", trajectory)\n",
    "print(\"\\nRotation at each step:\\n\", rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeTrajectory(trajectory):\n",
    "    \"\"\"Show a 3D plot of the trajectory\"\"\"\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    ax.plot(trajectory[:,0], \\\n",
    "            trajectory[:,1], \\\n",
    "            trajectory[:,2])\n",
    "\n",
    "    ax.set_xlabel(\"$x$\")\n",
    "    ax.set_ylabel(\"$y$\")\n",
    "    ax.set_zlabel(\"$z$\")\n",
    "\n",
    "    ax.set_xlim(-20,20)\n",
    "    ax.set_ylim(-20,20)\n",
    "    ax.set_zlim(-20,20)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "860932c4186e4c718bb0fa8e8b9fffb0"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "visualizeTrajectory(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}